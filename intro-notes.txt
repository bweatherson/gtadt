A lot of contemporary decision theory revolves around what to do if there is a certain kind of demon around. Following @Nozick1969, such a demon is typically taken to be arbitrarily good at predicting what a human deliberator will do. I'll call our arbitary deliberator Chooser, and whenever X is a choice Chooser can make, I'll use PX to mean that the demon predicts Chooser makes that choice. It's not so common to have problems where there are two such demons around, but I'll make heavy use of them, and in such cases I'll be clear about whether PX means that the first or the second demon predicted that Chooser will do X. These are predictions, and we assume that causation runs from past to future, so what Chooser does has no causal impact on what Demon predicts. 

I'm squeamish about assigning probability 1 to predictions that are causally isolated from the thing being predicted; I have reductionist enough views about causation to think that if a prediction is correct with probability 1, that raises questions about whether causation does really run from past to future in this case. So I prefer to say that the Demon is correct with a probability close enough to 1 that it doesn't matter for the purposes of the problem being analysed. But this squeamishness, and the associated reductionism about causation, is not part of GDT. If you're happy with having causally isolated Demons who are correct with probability 1, everything else I say should be acceptable. Indeed, some of the reasoning goes through even more smoothly with perfectly accurate, but causally isolated, Demons.

A generic binary choice problem involving Chooser and Demon looks like this. [Note to self - have a callback to this in gad.qmd)

|       | PA  | PB  |
|:-----:|:---:|:---:|
| **A** | $x$ | $y$ |
| **B** | $z$ | $w$ |

: The demonic decision problem generated by a generic symmetric game. {#tbl-gen-dem-problem}

Chooser selects A or B, Demon predicts the choice, and there are four possible outcomes. I'll assume that the value of these outcomes can be measured numerically, with greater numbers being better. We'll come back to this assumption briefly in @sec-ideal, and more substantively in @sec-expect. Following @Nozick1969, the most common problem that people discuss involving Demon is what Nozick dubbed "Newcomb's Problem", after the physicist who suggested the problem to him. A Newcomb problem is an instance of @tbl-gen-dem-problem satisfying the following constraints.

- $z$ > $x$
- $w$ > $y$
- $x$ >> $w$

The standard example uses (more or less) the following values, but all that really matters are the three inequalities above.

|       | PA  | PB  |
|:-----:|:---:|:---:|
| **A** | 1000 | 0 |
| **B** | 1001 | 1 |

: Newcomb's Problem. {#tbl-newcomb}

Option A and B are typically called 'one-boxing' and 'two-boxing' respectively, because they involve selecting either one or two boxes in the vignette Nozick gives to go along with the story. But what really matters is the schematic form, not the details of the physical setup.

Nozick distinguishes two approaches to this problem you might take. He doesn't use the following terms, but they quickly became identified as Evidential Decision Theory, and Causal Decision Theory. Evidential Decision Theory says that the value of an option X for Chooser is given by taking the following sum. For each possible state, i.e., each prediction that may have been made, multiply the value of choosing X and being in that state, by the probability of being in that state given that X is chosen. Compute that product for each state, sum across the results, and that's the value of the action. So if Demon is 90% reliable, the value of A is 0.9 times 1000, plus 0.1 times 0, equals 900, and the value of B is 0.1 times 1001, plus 0.9 times 1, equals 101. Then Evidential Decision Theory says to prefer more valuable to less valuable options, so it says the choose option A.

Causal Decision Theory, on the other hand, is moved by the following argument. Whatever Demon has predicted, Chooser is better off choosing B than A. That, says Causal Decision Theory, settles things; Chooser should take option B. I think this is right; Chooser should choose B, and they should do so for this reason. But note that this is not anything like a complete theory of choice. Two people could agree with this little argument and have any number of different views about problems that not so easily disposed of. In this book, especially in @sec-indecisive, we'll spend a lot of time on problems like the following.

|       | PA  | PB  |
|:-----:|:---:|:---:|
| **A** | 6 | 0 |
| **B** | 5 | 2 |

: The Stag Decision.^[I say much more about why the problem has this label in @sec-gad.] {#tbl-stag-decision-first}

It turns out that among people who endorse the little argument for choosing B in @tbl-newcomb, there are at least four distinct views about what to do in @tbl-stag-decision-first.

1. Frank @Arntzenius2008 and Johan @Gustaffson2011 recommend Choosing A.
2. Ralph @Wedgwood2012, Dmitri @Gallow2020, Abelard @Podgorski2022, and David @Barnett2022 recommend choosing B.
3. James @Joyce2012 says that what Chooser should do is a function of Chooser's probability distribution over their choices prior to deliberating about what to do.
4. Jack @Spencer2022 and Melissa @Fuscond say that Chooser can rationally take either option.

I'm going to side with option 4. Though note that Spencer and Fusco disagree about what Chooser should do in several other cases, most notably in cases like @tbl-stag-decision-first but with the payouts inverted. (For what it's worth, I'm going to be more or less on Fusco's side on these disputes, though with a small difference in emphasis.)

But I'm not going to argue for option 4, or against any other options, just yet. Rather, I want to start with a terminological point. It's not obvious, either from the decsription of the problems or the history of the philosophical discussion, which if any of these theories should be called Causal Decision Theory. Some people write as if Joyce's view is the unique one that should get that name; indeed many of the people I've listed above describe themselves as critics of Causal Decision Theory who are offering an alternative to it. I think that's not the most helpful way to classify views. All of them accept that in @tbl-newcomb Chooser should choose option B, and that Chooser should choose it because Chooser can't make a causal difference to whether PA or PB happens, and either way, B is better than A. That's the core idea behind Causal Decision Theory.

- Not a theory.
- Lots of ways of spelling out.
- Better to save "Causal Decision Theory" for the family, and use other terms for the more particular views.
- This means "Causal Decision Theory" is not a theory, but hey we're used to that.
- In any case, that's how I'm using the term - any theory that recommends B because it is causally dominant is a causal decision theory. Capital letters Causal Decision Theory is the name of the family of views. The bulk of this book will be an in-house dispute between these theories, though I will have one somewhat novel argument against EDT.