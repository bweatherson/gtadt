# Dual Mandate {#sec-dual}

## Introduction {#sec-dual-introduction}

This chapter marks a turning point in the book. From here on, a large amount of the time will be spent discussion situations involving dynamic choice. A central argument for GDT is that only it, or something like it, is compatible with plausible principles of dynamic choice.

The aim of this chapter is to defend what I call the **Dual Mandate**. In dynamic choice situations, rational Chooser will act in such a way that (a) each individual choice they make is sensible, and (b) the choices they make collectively are sensible. And I'll defend the claim that (a) and (b) are distinct constraints on rational Chooser. This position is, I think, fairly orthodox in game theory. But it is not, I believe, widely held in philosophical decision theory. The caveat there is because a lot of decision theorists do not discuss dynamic choice, so I can't always tell what their view is. But of the ones that do, most either reject (a), saying that only collective choices are to be evaluated, reject (b), saying that only individual choices are to be evaluated, or deny that these are distinct constraints. The middle option is by far the most common, but you can easily enough find examples of all three. It's somewhat harder to find examples of people in the philosophy literature endorsing the Dual Mandate.

For related reasons, this chapter will be longer and more technical than what came before. Even saying what the Dual Mandate says involves some setup. So this section will largely be about defining the key terms.

### Decision Trees {#sec-decision-tree}

I'll call the dynamic choice situations I'll be interested in decision trees. To define them, it helps to start with an orthodox definition of a game tree.

> A finite extensive form (or frame) with perfect recall consists of the following items.\
> - A finite rooted directed tree.[^dual-1] - A set of players *I* = {1,...,n} and a function that assigns one player to every decision node. A set of actions *A* and a function that assigns one action to every directed edge, satisfying the restriction that no two edges out of the same node are assigned the same action. A set of outcomes_O\_ and a function that assigns an outcome to every terminal node. For every player *i* $\in$ *I*, a partition $\mathfrak{D}_i$ of the set *D~i~* of decision nodes assigned to player *i* (thus $\mathfrak{D}_i$ is a collection of mutually disjoint subsets of *D~i~* whose union is equal to *D~i~*). Each element of Di is called an information set of player *i*. [@Bonanno2018, 119]

[^dual-1]: This is defined earlier, on page 75, but the details aren't important to what we're doing.

The quote continues with some restrictions on $\mathfrak{D}_i$, but I want to pause first to say what this is supposed to represent, and then it is easy to say informally what the constraints are. This partition is an epistemic accessibility relation. If two nodes are in the same cell of the partition, then when the player is in one of them, for all they know, they are in the other. The strongest thing they know, when they are at a particular node, is that they are somewhere in that cell of the partition.

Implicitly, the assumption here is that the right accessibility relation for epistemic logic is an equivalence relation. That's absurd in full generality. But I think in this context it's a harmless enough idealisation. That is, it's harmless enough if we remember that an idealisation here is a simplification, and not something that we think is desirable, or in any way something to aim for.

There are two standard restrictions on $\mathfrak{D}_i$. First, players know what moves are possible, so for any two nodes in a cell of $\mathfrak{D}_i$, the same actions are possible. Second, players remember their actions, so for any two nodes in a cell of $\mathfrak{D}_i$, the paths to those nodes only differ with respect to moves made by other players.

We're also going to make three more assumptions that are I think implicit in the standard formulation, but not always made explicit.[^dual-2] Say a 'play' is a particular path through the tree that happens in real time. The assumptions concern what happens in all plays of a tree thus understood.

[^dual-2]: Bonanno does make all these explicit at various times, but doesn't list them in one spot for neat quoting.

First, each player is motivated to get the best outcome possible. If we interpret the outcomes as preferences of the player at the end of the play, and assume that players are motivated by their current preferences, this is in effect an assumption that preferences do not change over the course of the play.

Second, the tree is common knowledge at the start of the play. A player will not acquire new capacities over the game, or learn that they had capacities they didn't realise. They will not acquire any capacity to make distinctions between possibilities that they did not have at the start of the play. So, for instance, there can't be a point in the tree where a player meets a new individual, and acquires by acquaintance the ability to have singular thoughts about that person, and distinguish that person from descriptive duplicates.[^dual-3]

[^dual-3]: Following @Stalnaker2008, I think this constraint means that we can't represent the Sleeping Beauty problem (@Elga2001) as a tree, since in that problem Beauty gains the capacity to have singular thoughts about a time, the 'now' when she awakes, that she did not previously have.

Third, it is common knowledge among all the time-slices of a player that all of the player's time-slices are rational. At this stage, it's important that 'rational' be left as something of a placeholder, or, perhaps better, a variable. In some sense the aim of the theorising around here is to solve for the value of 'rational' given some intuitive data about rational play. But whatever rationality is, we assume the player always has it, they always know they will always have it, they always know that they always know they will always have it, and so on.

### Strategies {#sec-strategies}

A *strategy* for player *i* is a function from the members of $\mathfrak{D}_i$ to probability distributions over actions. That is, the strategy says which action, or mixed action, a player will do at each information set that they could reach consistent with the rules of the game.

It will become important that these are extremely fine-grained. A strategy describes what a player will do at nodes that are ruled out by other choices the player makes. Consider a game that consists of two rounds of some simple two-player, two-choice game, like Prisoners' Dilemma, with the results of the first round revealed before the second round is played. Each player has 32 strategies. (So there are 1024 strategy pairs.) The tree for the game has five information sets where each player might move; the first-round game, then, since there are four ways the first game could go, four more possibilities for what they might know when the second-round game is played. Since there are 2~5~, i.e., 32, ways to make binary choices over five possible choices, there are 32 strategies.[^dual-4]

[^dual-4]: Some of the results of the next few chapters came from work I started investigating what happened in two-round decision problems like that. None of that work appears here, because for every result I found, I eventually found an illustration with many fewer strategies. If you're grateful you don't have to look at 32-by-32 strategy tables, you can't imagine how grateful I am to not be writing them.

I'm going to assume a kind of realism about strategies. Players actually have dispositions about what they will do at nodes that aren't reached, and even at nodes that couldn't be reached given their prior dispositions. These dispositions are at least real enough to play the following two roles: they can be the conditions that conditional probabilities are defined over, and they are subject to evaluation as rational or irrational.

### Special Players {#sec-special-players}

Trees in game theory textbooks frequently designate one special player: Nature [@Bonanno2018, 134ff]. Nature is different in two key respects.

Nature does not care about which outcome the game ends with; formally, we describe an outcome by listing the utilities for the players other than Nature.

Nature is not rational. It is usually taken to be common knowledge among the other players that they are rational; but Nature is treated differently. Rather, at every node where Nature makes a move, there is an externally provided probability distribution over the possible moves. These probabilities are common knowledge at the start of the game.[^dual-5]

[^dual-5]: Though note that does not mean all players know the probability of each move at any time Nature moves. It could be that while the game is going, a player does not know precisely which node they are at, so they do not know what probability distribution Nature is using. This is common in card games. If I don't know what's in your hand, I don't know what cards are left, so I don't know whether the probability that Nature is about to give a player the Jack of Hearts is, say, 0.025, or 0.

The key formal move here is to introduce new players, demons, which behave a bit like rational players, and a bit like Nature.

A **decision tree**, as I'll understand it in this book, is like a game tree, but with more players that are distinctive in the way Nature is. There is only one player stipulated to be rational: Chooser. (They will be player 1 in what follows, unless stated otherwise.) At most one player is Nature, in the game-theoretic sense. The other players are all demons. If a demon moves at a node, then Chooser knows not the unconditional probability of that demon's possible actions, but the conditional probability of the demon's actions given their strategy choice. In more familiar terms, the demon predicts their strategy with a certain probability of accuracy, and has dispositions about what to do given each prediction.

While these demons are a lot like the demons that have been central to decision theory ever since the introduction of Newcomb's Problem, there are two things I'm doing differently here that I want to note up front. First, there may be more than one demon. In the examples to follow, there will occasionally be four players: Chooser, two demons, and Nature. Second, the conditional probabilities are conditional on strategies, not just choices. This will matter in two stage games; to make the second stage game be just like the familiar games in decision theory (like Newcomb's Problem), it will be important that Demon's dispositions are sensitive to Chooser's dispositions about the second game. And this is important even in cases (of which there will be a few below) where Chooser can choose whether to play that second-round game.

### Extensive Form and Strategic Form {#sec-equivalence-intro}

Given a decision tree, we can generate a related game where each player has precisely one choice: what strategy they will play. This sometimes called the **strategic form** of the game, and sometimes called the **normal form**. I'll primarily use the earlier, more evocative, term.[^dual-6] The contrast, the decision tree where the players act over time, is called the **extensive form** of the game. I said the strategic form of a game is to its extensive form, but you might wonder how closely related it is. Is it, in some sense, the same game?

[^dual-6]: Whenever I come back to this material after time away, I can never remember what 'normal form' means. But it's easy to remember that extensive form is extended in time, and strategic form is about strategy choice.

One way to make progress on this question is to ask whether the strategic form and extensive form are equivalent, in the sense that the following thesis is true.

Strategic Form - Extensive Form Equivalence

:   Some moves in an extensive form of a decision tree are rational (both individually and collectively) iff they are part of some strategy that can be rationally played in the corresponding strategic form decision.

Most game theorists deny this equivalence.^[The next few paragraphs are based on the game theoretic notion of non-credible threats [@Bonanno2018, 86ff].] The examples used to motivate this are fairly simple. In @fig-subgame-example, Demon moves first, and Chooser moves second. Chooser can play A or B, Demon can play PA or PB. Demon wants these predictions to be correct. Chooser gets a reward iff Demon's prediction is correct. Chooser gets a higher reward if they both choose A than if they both choose B. Demon is arbitrarily good at predicting Chooser's [strategy](#sec-strategies}, and this is common knowledge to both players. Demon will do whatever makes it most likely that their prediction is correct, or flip a coin if their choice does not affect the probability that they will make a correct prediction. 

```{r engine='tikz'}
#| label: fig-subgame-example
#| fig.cap: "Tree Diagram of the Non-Equivalence Game."
#| fig.ext: 'png'
#| cache: TRUE
#| echo: FALSE
#| fig.width: 4
\usetikzlibrary{calc}

\begin{tikzpicture}[scale=1.4,font=\footnotesize]
\tikzset{
% Two node styles for game trees: solid and hollow
solid node/.style={circle,draw,inner sep=1.5,fill=black},
hollow node/.style={circle,draw,inner sep=1.5},
square node/.style={rectangle,draw, inner sep = 1, fill = black}
}

% Specify spacing for each level of the tree
\tikzstyle{level 1}=[level distance=10mm,sibling distance=20mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=10mm]
\tikzstyle{level 3}=[level distance=20mm,sibling distance=20mm]

% The Tree
  \node[hollow node,label=above:{Demon}]{}
    child {
      node[solid node,label=left:{Chooser}]{}
      child {
        node[square node,label=below:{2}]{}
        edge from parent node[left] {A}
      }
      child {
        node[square node,label=below:{0}]{}
        edge from parent node[right] {B}
      }
      edge from parent node[left] {A}
    }
    child {
      node[solid node,label=right:{Chooser}]{}
      child {
        node[square node,label=below:{0}]{}
        edge from parent node[left] {A}
      }
      child {
        node[square node,label=below:{1}]{}
        edge from parent node[right] {B}
      }
      edge from parent node[right] {B}
    };
\end{tikzpicture}
```

Here is how to understand graphs like @fig-subgame-example. The circles are nodes where one or other player (Chooser, Demon, or Nature) has to make a choice. The open circle, here at the top of the tree, is the first such choice. Where possible, I'll draw trees where later choices are lower on the page than earlier choices, but this isn't always possible. What is always the case is that the open circle is the opening move. The small square nodes are terminal nodes; at that point the game ends, and Chooser collects their payout.

The strategic form of this game is given in @tbl-subgame-example. Demon clearly has two strategies, PA and PB. But Chooser has four; since they have to plan for a binary choice in two possibilities. I've written LXRY for the strategy of doing X on the left hand part of the tree, i.e., if Demon predicts A, and doing Y on the right hand part of the tree, i.e., if Demon predicts B. 

|          | PA   | PB   |
|:--------:|:----:|:----:|
| **LARA** |  2   |  0   |
| **LARB** |  2   |  1   |
| **LBRA** |  0   |  0   |
| **LBRB** |  0   |  1   |

: Strategic form of the Non-Equivalence Game {#tbl-subgame-example}

The argument against Strategic Form - Extensive Form Equivalence is now fairly simple. In @fig-subgame-example, there is only one rational choice: LARB. Whatever happens, Chooser has an option between getting something and getting nothing, and it's better to get something than nothing. But in @tbl-subgame-example, there are many rational choices. The pair of Chooser playing LARA and Demon playing PA is a Nash equilibrium. If one's theory of rational choice for strategic games is that any Nash equilibrium is rational, then playing LARA in @tbl-subgame-example is rational. Hence different strategies are rational in @fig-subgame-example and @tbl-subgame-example, so Equivalence fails.

There are a bunch of ways one could reply to this. One could argue that in fact LARA is rational in @fig-subgame-example. We'll see a theory that says that in @sec-purely-strategic. One could argue that LARB is the only rational play in @tab-subgame-example. And this complicates the dialectic here, because while GDT _rejects_ Equivalence, it also rejects this example. It agrees that LARB is the only rational choice in @tab-subgame-example, because it weakly dominates LARA.^[I'll discuss this more at length in @sec-weak.] The examples that motivate rejecting Equivalence within GDT are more complicated, and I'll come back to them in @sec-against-equivalence.

Note that it's not enough to merely deny that there are multiple permissible moves in @tbl-subgame-example. Evidential Decision Theory says that there is only one rational move in that game, and it's LARA. That has an expected return of 2, while LARB has an expected return of 1.5. So this does work as a counterexample to Equivalence within standard forms of EDT, though not for the same reason that the game theorists think it is a counterexample.

## Four Options {#sec-four-options}

### Introducing the Contestants {#sec-contestants}

I've already briefly alluded to these, but it's time to set out in more detail the four approaches to dynamic choice that I'll consider at greater length.

**Purely Strategic** approaches say that Chooser uses decision theory to choose a strategy, and then implements that strategy at each node. This is sometimes known in philosophy as the resolute approach to decision theory.^[That particular term, 'resolute', is associated with a view that Edward McClennen developed to deal with cases of foreseeable changes of preference [@McClennen1990]. As already indicated, I'm just looking at games where preferences do not change over the game.] In a finite game, there will be finitely many strategies. This finite number may be very large, but it can be calculated. And similarly there are finitely many strategies for each of the non-human players, and Chooser can work out the conditional probability for each such strategy given their choice of strategy. So we have a very large, but finite, game, and most decision theories on the market in philosophy will have something to say about what Chooser should do in this large game. All that's then left to do is to carry the strategy out. 

**Purely Consequentialist** approaches say that Chooser will every decision on its own merits, solely thinking about the consequences of that particular decision. This is sometimes called the 'sophisticated' approach to dynamic choice in philosophical discussions, but I prefer calling it the Purely Consequentialist approach. I don't like the implicit endorsement in calling something sophisticated, and given that I've already called the rival approach strategic, having two names starting with the same letter is bad. This name echoes the influential understanding of consequentialism in @Hammond1988. And it gets at what is important about the view; that it rules out looking back.

If Chooser is Purely Consequentialist and they face a series of choices, they will work backwards.^[See the discussion of backward induction on pages 80ff of @Bonanno2018.] They will work out what they will do at terminal stages of the game, i.e., at stages where they will have no more decisions whatever they do. When they are making a decision at a non-terminal stage, they will treat their own future decisions as something to be predicted, not planned for. So they will have a probability distribution over the possible choices, and act as if Nature is (randomly) selecting which choice. Now we've stipulated that Chooser knows they will be rational in future stages, so in cases where there is only one rational choice, Chooser will assign probability 1 to them making that choice, and 0 to the alternatives. But in the cases where there are multiple options that are rationally permissible, this probability assignment might be more interesting, and I'll have more to say in @sec-against-pure-consequentialist about the effects of this.

**Equivalence** approaches say that Chooser does not have to adopt one or other of these approaches, because once we have the right theory of choice in one-shot games, it will turn out that the two approaches issue in the same verdicts. That is, it will turn out that Strategic Form - Normal Form Equivalence is true. Robert @Stalnaker19xx defends this view in game theory. I'll argue that the differences between demonic decisions problems and games are just big enough that his defence can't be adopted to the puzzles I'm looking at.^[This is just about the only place in the book where I'll rely on a _disanalogy_ between decision theory and game theory.]

Finally, the **Dual Mandate** approach says that both of the first two approaches were partly correct. Both of them correctly state necessary conditions for rational action. Where they go wrong is the 'pure' part, i.e., by saying that these are sufficient conditions for rational action. And that's what GDT says, and what I'm going to defend.

### Picturing the Views {#sec-four-pictures}

Chooser has to act now, and which action is best depends in part on what they'll do tomorrow? How should they think of tomorrow's action?

Not as something they can control; they are a free and rational agent, who can't simply be bound.

Not as something like the weather that they can merely predict; they don't see themselves as simply a thing to be predicted.

Quote Stalnaker at length, can't remember which paper.

Agree - we need something in between.

Strategic views as Strangelove; I bind myself into a position, the position that I would most like to be bound into.

Consequentialist views as Zaphod; future me is just this guy.

Neither is good.

This has an important technical consequence: Purely Consequentialist theories are *unstable* in the sense Dmitri @Gallownd has described. If the pure consequentialist changes their own probability distribution over what they will do, what act is rational for them changes.[^dual-oct-5-23-6] I don't think this is ever an issue for CEDT, but it is an issue for other Purely Consequentialist theories. For instance, GDT says that either A or B is a permissible choice in @tbl-unstable-example.

[^dual-oct-5-23-6]: The point here is somewhat connected to the point Bonanno makes about how backwards induction works in games where a player is indifferent between certain outcomes [@Bonanno2018, 80ff].