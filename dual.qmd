# Dual Mandate {#sec-dualmandate}

Say a decision tree is a series of steps with the following characteristics.

-   At every step, Chooser either receives some information, or makes a choice.
-   Chooser knows before the first step what possible choices will be available at each step, given the prior steps, or what possible pieces of information could be received.
-   No matter what happens, the tree ends after finitely many steps. (Though it may end after more or fewer steps depending on what happens).
-   Chooser knows before the first step what payout they will receive given each possible sequence of choices and information.
-   Before the first step, chooser has a probability for each possible piece of information they could receive, given the prior steps in the tree.

That's incredibly abstract, but it excludes some possibilities. It excludes cases where Chooser learns along the way that they have hitherto unknown abilities. It excludes cases where Chooser gains the capacity to think new relevant thoughts along the way, say by meeting a new person and gaining the capacity to have singular thoughts about them.[^dual-1] Still, it does cover a lot of cases.

[^dual-1]: Following @Stalnaker2008, I think it excludes the Sleeping Beauty case, since there Beauty gains the capacity to have singular thoughts about a time, the 'now' when she awakes, that she did not previously have.

Say a strategy for a decision tree is a plan for what to do in every possible choice situation. Following the game theory textbooks, I really do mean *every* here. A strategy should say what to do in cases that are ruled out by Chooser's prior choices. A strategy for playing chess as White might say to start with e4, but also include plans for what to do if you inexplicably start Na3. There are both mathematical and philosophical reasons for having such an expansive conception of strategies, and I'll come back to one reason why later in this chapter.

Here are two classes of philosophical views about how decision makers facing such trees should act. 

One approach is what I'll call the **purely strategic** approach. This says that Chooser should treat the problem as one of making a one-off choice between the (potentially very large) number of strategies. Then at each step, an act is permissible iff it is part of the strategy Chooser has permissibly chosen. On this view, the only difference between a static choice, where Chooser just makes a decision, finds out something about the world, and gets a return, and a dynamic choice, like when Chooser faces a tree like this, is that in the latter Chooser has to implement their choice in stages.

Another approach is what I'll call the **purely consequentialist** approach. This says that at each time Chooser has to make a decision, they are facing a wholly new decision problem, and the only rational constraints they face are ones that they would be facing if they were at the start of a tree. Moreover, Chooser knows now that their future self will be like this. So when they now make decisions whose consequences turn on (among other things) what their future self will do, they regard those decisions by their future self as matters for prediction, not planning. They take the same attitude towards their future self as they take to other decision makers. That means that they may assume (if it's given as an assumption) that they will be rational, and if there are multiple possible rational choices, they can have a probability distribution over the choices the person may make, but they don't regard themselves as in a position to bind their future self.

I'm going to reject both of these approaches. My preferred approach follows what I take the orthodoxy in game theory to be. This view follows the lead suggested by @Selten1965 which says in effect that both the strategic and consequentialist approaches are correct. But because they are both correct, it can't be right to have a _purely_ strategic, or a _purely_ consequentialist view. I'm going to call this the **dual mandate** approach. More formally, it says that in such a tree, every choice Chooser makes should be defensible on consequentialist grounds, and the set of choices that Chooser makes should all be part of some strategy that is defensible on strategic grounds. Chooser's choices must make sense taken one-at-a-time, and they must make sense taken collectively. As I said, I think this is more or less game-theoretic orthodoxy, but I don't think it's a common position in the philosophical literature.

The three options from the last three paragraphs do not exhaust the field. There are views that reject the way I have set up the problem. In the first bullet point, I said that Chooser might either receive information, or make a choice. In a game, there are two kinds of information Chooser might receive. Either a genuinely random possibility might get resolved some way, or a rational player might act.^[Game theorists typically refer to the first as 'Nature' making a move.] One could have a theory that treated the two possibilities differently. For instance, I can see the appeal of having a view that was more strategic when it came to interactions with others, but more consequentialist when it came to interactions with purely random parts of the world.^[For people familiar with the decision theory literature, this kind of position may be appealing to those whose intuitions supported taking one box in Newcomb's Problem, but supported taking the causally dominant choice in "Medical Newcomb Problems" (to borrow a phrase from Huw @Price1991).] My arguments below against the purely strategic approach are meant to extend to these moderately strategic approaches, but it's impossible to predict every possible move that might be made, and I suspect there are some interesting approaches on these lines that aren't ruled out, even if everything I say here is true.

In philosophy, the purely strategic approach is often called the resolute approach[^dual-2], and the purely consequentialist approach is called the sophisticated approach. But I don't think it's too bad to introduce new names here. My use of consequentialist follows the very influential use of @Hammond1988, and it's useful to connect the strategic approach to game-theoretic strategies.

[^dual-2]: Most notably defended by @McClennan1990.

The orthodoxy in game theory, going back to at least @Selten1965, is that both views are correct. When faced with a decision tree, Chooser should follow the advice of the sophisticated theorists, and (given they are ideally rational) do what would be best on the assumption that future choices will be rational. But in doing so, they should instantiate (part of) a strategy that could be rationally chosen by the resolute chooser. I call this the Dual Mandate approach, and I am going to defend it.

Start with why it is bad to just have a resolute approach.[^dual-3] Game theorists usually reject this approach because it means sometimes making a decision that one knows will have worse consequences than an available alternative. I'll go over an example of this, though I should note it is rather violent. This is unavoidable; it is only in these violent cases that we can be sure the Chooser is really making things worse, and not acting for a strategic or reputational goal.

[^dual-3]: The so-called Foundational Decision Theory of @LevinsteinSoares2020 agrees with the resolute approach in the special case where the only information Chooser will receive are the results of predictions, and is subject to the criticisms I'll make of resolute theories.

Chooser is the Prime Minister of a small country, and they are threatened by a large neighbour. Unfortunately, neighbour is thinking of carpet bombing Chooser's capital, in retaliation for some perceived slight. Chooser has no air defences that would prevent a great destruction, and no allies who will rally to help. Fortunately, Chooser has a mighty weapon, a Doomsday device, that could destroy neighbour. Chooser has obviously threatened to use this, but neighbour suspects it is a bluff. This is for a good reason; the doomsday device would also destroy Chooser's own country. Neighbour is known to employ a Demon who is at least 99% accurate in predicting what military plans Chooser will take. So Chooser can do Nothing (N), or use the Doomsday device (D), should neighbour attack. Chooser would obviously prefer no attack, and would certainly not use the device preemptively. So here is the table.

|       |        |        |
|:-----:|:------:|:------:|
|       | **PN** | **PD** |
| **N** |   -1   |   0    |
| **D** |  -50   |  -50   |

: Deciding whether to retaliate. {#tbl-retaliation}

In the top left, neighbour bombs Chooser's capital, thinking correctly that Chooser will not retaliate. In the top right and lower right, neighbour is sufficiently scared of the doomsday device that they do nothing. But in the bottom left, neighbour attacks, and Chooser retaliates, creating a disaster for everyone, something 50 times worse than even the horrors of the carpet bombing.

Still, if Chooser is picking a strategy before anything starts, the strategy with the highest expected return is to plan to retaliate. This has an expected return of -0.5; since one time in a hundred it returns -50, and otherwise it returns 0. The resolute theorist says that's what Chooser should do, even if they see the bombers coming, and they realise their bluff has failed. This seems absurd to me, and it is the kind of result that drives game theorists to the dual mandate, but resolute theorists are familiar with the point that their theory says that sometimes one should carry out a plan now known to be pointless. So instead of resting on this case, as decisive as it seems to many, I'll run through two more arguments against a purely resolute theory.

Change the example so that Chooser has two advisors who are talking to him as the bombers come in. One of them says that the Demon is 99% reliable. The other says that the Demon is 97% reliable. Whether Chooser launches the doomsday device should, according to the resolute theorist, depend on which advisor Chooser believes. This is just absurd. A debate about the general accuracy of a demon can't possibly be what these grave military decisions are based on.

Change the example again, and make it a bit more realistic. Chooser has the same two advisors, with the same views. Chooser thinks the one who says the Demon is 99% reliable is 60% likely to be right, and the other 40% likely. So Chooser forms the plan to retaliate, because right now that's the strategy with highest expected return. But now, to everyone's surprise, neighbour attacks. The resolute theorist will say that Chooser should stick to their (overpowered) guns. But think about how the choice of plans looks to Chooser now. The actions of neighbour are evidence about the reliability of the demon. And a simple application of Bayes' Rule says that Chooser should now think the advisor who thought the demon was 97% reliable is 2/3 likely to be right. That is, given Chooser's current evidence, retaliating wasn't even the utility maximising strategy to start with. Yet it is what the resolute theorist, or at least the resolute theorist who is not also sophisticated, would have Chooser do. This is, again, absurd, and enough reason to give up on such a theory.

What about the other direction? Is it sensible to have a sophisticated theory that is not resolute? There does seem to be something puzzling about such theories. They are "diachronically exploitable" in the sense described by @Spencer2021. Let's start with one example. Extend the theory offered by @Gallow2020 to make it a pure sophisticated dynamic theory. That is, in a decision tree, the chooser values future choices by the expected value of the choice they'll make, and if that choice is guaranteed to end the decision tree, they use Gallow's theory. Chooser is now offered the following two-step option. At step 1 they can choose to receive 1 or play the game in @tbl-gallow-sophisticated.

|       |        |        |
|:-----:|:------:|:------:|
|       | **PU** | **PD** |
| **U** |   2    |   2    |
| **D** |   5    |   0    |

: A challenge for pure sophisticated decision. {#tbl-gallow-sophisticated}

If Chooser gets to step 2, they'll play D, since it is the best option according to Gallow's theory. So at step 1 they'll choose the 1 rather than playing this game. But that's absurd; they know they could have done better by simply playing the game and choosing U.

What the Dual Mandate says is that the last step of reasoning here is sound; it is a fair criticism of an agent to say that their strategy doesn't make sense even if every step makes sense taken on its own. Since this does seem like a fair criticism, it is reasonable to adopt the Dual Mandate.

If one has a decisive theory, then a huge number of decision trees will be dilemmas, since it is unlikely that the optimal strategy matches the series of optimal choices. This is not a reason to reject the Dual Mandate; it's another reason to reject decisiveness.

You might worry that the argument based around @tbl-gallow-sophisticated is not really an objection to theories that reject the Dual Mandate, but just to the combination of that rejection and the endorsement of Gallow's particular theory of decision. That worry is half right. This result is a problem for Gallow's theory. But that doesn't mean it isn't also an argument for the Dual Mandate. The point of the Dual Mandate is not to criticise individual decisions, like taking the 1 in this game. It's to criticise theories that endorse those decisions. It's true that once we find the right theory of synchronic choice, the Dual Mandate will be unnecessary, since it will be automatically satisfied.^[IMPORTANT NOTE TO SELF: This isn't right. In cases where there are multiple equilibria, earlier choices might rule out some later choices. E.g., when there is an exit choice that is guaranteed to be better than some earlier choice. Gotta fix all this.] But the Dual Mandate plays an essential role in selecting that theory.

Jack @Spencer2021 has an example which he thinks tells against the Dual Mandate, or what he calls the requirement that Chooser not be diachronically exploitable.[^dual-4] The agent will play first the left and then the right game, and their payouts (shown in dollars) will be summed over the game. They won't be told between the games what they got from the first game.[^dual-5]

[^dual-4]: Spencer's non-exploitability isn't quite the same thing as the Dual Mandate, but it's close enough for these purposes. Spencer rejects non-exploitability, but endorses a weaker constraint he calls the Guaranteed Principle. I don't see any reason to distinguish between these constraints, in part because of the argument that follows in the text.

[^dual-5]: Assume Chooser is reasonably risk-neutral over dollars over this range of outcomes.

::: {#tbl-newcomb-insurance layout-ncol="2"}
|          |           |           |
|:--------:|:---------:|:---------:|
|          | **PU~1~** | **PD~1~** |
| **U~1~** |    50     |    -50    |
| **D~1~** |    60     |    -40    |

: First game {#tbl-ni-left}

|          |             |               |
|:--------:|:-----------:|:-------------:|
|          | **Correct** | **Incorrect** |
| **U~2~** |     25      |      -75      |
| **D~2~** |     -25     |      75       |

: Second game {#tbl-ni-right}

Ahmed Insurance (from @Spencer2021).
:::

Note that in @tbl-ni-right, the states are not the usual ones about Demon's predictions. Rather, they are that the Demon made the Correct, or Incorrect, prediction in @tbl-ni-left. There are eight strategies in this game, but since the Demon doesn't care about what happens at non-chosen nodes, we won't care either, and just focus on the four combinations of moves Chooser might make, and how they interact with Demon's prediction. If we do that, we get the following table (also given by Spencer, and also with payouts in dollars).

|              |           |           |
|:------------:|:---------:|:---------:|
|              | **PU~1~** | **PD~1~** |
| **U~1~U~2~** |    75     |   -125    |
| **U~1~D~2~** |    25     |    25     |
| **D~1~U~2~** |    -15    |    -15    |
| **D~1~D~2~** |    135    |    -65    |

: Strategic form of Ahmed Insurance. {#tbl-ni-strategic}

Spencer argues that even though D~1~U~2~ is dominated by U~1~D~2~ it might be rational to play it. After all, it is rational to bet on Demon being correct in @tbl-ni-right, since Demon is arbitrarily good. And if one knows one is going to do that, one may as well take the sure extra \$10 that playing U~1~ rather than D~1~ gives. So diachronic exploitability is consistent with rationality.

The reasoning of the previous paragraph fails because neither CDT, nor any other sensible decision theory, recommends taking two boxes in Newcomb Problems embedded in strategic interactions. This would be like thinking that CDT recommended always defecting in Iterated Prisoners' Dilemma, even it was chancy whether the iterations came to an end after each round, so backward induction reasoning was unavailable. If Chooser has convinced themselves that they will play U~2~, and we'll come back to whether they should believe that, then the choice in @tbl-ni-left comes down to this.

|          |           |           |
|:--------:|:---------:|:---------:|
|          | **PU~1~** | **PD~1~** |
| **U~1~** |    75     |   -125    |
| **D~1~** |    -15    |    -15    |

: First game in Ahmed Insurance, if D~2~ will be played. {#tbl-ni-new-left}

This game has two pure strategy equilibria, and on its own I think (because of the arguments in @sec-indecisive) that either play is acceptable. In context though, either play is clearly unacceptable. Given that one chooses either U~1~ or D~1~, the only reasonable thing to believe is that Demon has almost certainly predicted this, so it makes to play U~2~, since Demon is almost certainly correct. So one ends up playing U~1~U~2~ or D~1~U~2~, both of which are dominated and hence absurd strategies.

Spencer argues that since the Demon is almost certainly accurate, Chooser should play U~2~, so they should play a dominated strategy, so the Dual Mandate doesn't apply. (This assumes that synchronic choice rules out strictly dominated options in cases like this, but Spencer agrees that it does.) This argument only goes through if Chooser doesn't have access to mixed strategies; i.e., if Chooser is not ideally practically rational. If Chooser does have access to mixed strategies, they should play a 50/50 mix of U~1~ and D~1~, then choose D~2~. That is ratifiable as long as Chooser believes Demon plays PU~1~ with probability 0.45, and PD~1~ with probability 0.55. Since that's the only ratifiable play for Demon, it's reasonable for Chooser to believe this. If mixed strategies are allowed, this is not a case where the Dual Mandate fails.

In general, if mixed strategies are not allowed, the Dual Mandate is implausible. But that's because without mixed strategies, cases like @tbl-newcomb-insurance are dilemmas; they have no ratifiable choices. And it's true that the Dual Mandate is implausible in dilemmas. Think back to the sinner described in @sec-ratify. Imagine that sinner will in fact say that they get *d* days in heaven. Now complicate the case; they are offered a choice of *d*! days in heaven, or to make their own choice. If they will in fact choose *d*, they should simply take *d*!, even though there are strategies available, like choosing *d*!! days, that are better. Weird things happen when there are dilemmas around, and we shouldn't judge decision theories against these cases.

The Dual Mandate is also implausible if Chooser thinks they will be irrational, or that they will have different preferences. Indeed, it is implausible if Chooser thinks they might either change or lose their mind. For example, Odysseus binds himself to the mast because he does not approve of future-Odysseus's preferences. Professor Procrastinate[^dual-6] cite turns down a referee request because he does not trust his future self to be practically rational. Both of them deliberately turn down strategies that would be better than where they end up, because they do not trust their future selves to carry them out. They are alienated in this way from their future selves. When one does not endorse one's future preferences, or does not trust one's rationality in the future, it makes sense to be alienated from one's future self in this way. In such cases, one's future self is just another part of the world that must be predicted and worked around. And so it might make sense to forego, as Odysseus and Procrastinate forego, strategies that one's future self will not be so kind as to carry out.

[^dual-6]: A famous character in @JacksonPargetter1986.

My main claim here is when neither of those two conditions obtain, i.e., when one knows that one's future self will be rational and have the same preferences, one's choices should make strategic sense. That is, they should satisfy the fairly weak condition that they are part of some strategy that one could choose if one was simply choosing a strategy for the whole tree. Unless one fears future irrationality, or future change of preference, one should not be alienated from one's future self. If Chooser takes 1 rather than play @tbl-gallow-sophisticated, they are alienated in this way. They have to think, I know I'd be better off if I played U. But that fool future-me will play D instead, and blow up the plan. But future-them is not a fool; by hypothesis they are known to be ideally rational. So it isn't coherent to think this way, and that reveals that it is incoherent to 'rationally' take the 1. And that is why the Dual Mandate requires that one's strategy be rational, and not just the moves that make up the strategy.
