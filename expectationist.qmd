# Expectationist {#sec-expect}

There is a strange split in contemporary decision theory. On the one hand, there are questions about the way to model attitudes to risk, largely organised around the challenge to orthodoxy from @Quiggin1982 and @BuchakRisk. On the other hand, there are questions about what to do in cases where the states are causally but not probabilistically independent of one's actions, with the central case being Newcomb's Problem. The strange split is that these two literatures have almost nothing in common.[^expectationist-1]

[^expectationist-1]: There is a survey article from a few years ago - @Elliot2019 - that has summaries of the then state-of-the-art on these two questions. And it makes it very striking how little the literatures on each of them overlap.

This split might seem to make sense when one reflects that there is no logical difficulty in endorsing any prominent answer to one set of questions with any prominent answer to the other set. But things get more difficult quickly. For one thing, one answer to questions about risk, what I'll call the expectationist answer, is universally assumed by people working on issues around Newcomb's Problem. For another, the argument forms used in the two debates are similar, and that should affect how the two arguments go. 

Say that a normal decision problem is one where the states are probabilistically independent of the choices. A simple example is betting on a coin flip. In talking about normal decision problems I'll normally label the states H, for Heads, or T for Tails. Unless otherwise stated coins are fair, so H and T are equiprobable. And say that an abnormal decision problem is simply one that isn't normal. A simple example is where the states are predictions of an arbitrarily accurate predictor.

The view I call expectationism has two parts. First, it says that in normal decision problems, the rational agent maximises the expected value of something like the value of their action. So if the states are H and T, the expected value of a choice X is V(XH)Pr(H) + V(XT)Pr(T), and the expectationist says to choose the choice with the highest expected value.[^expectationist-2] Second, it says that something like this expected value plays an important role in the theory of abnormal decision problems. What's an important role is vague, so there are possible borderline cases. But in practice this doesn't arise, at least in the philosophy literature. Everyone working on abnormal problems is an expectationist. Indeed, most work assumes without even saying it that the first clause of expectationism is correct. Everyone working on normal problems makes it clear which side they fall on, so there is no vagueness there. And every game theory text is expectationist. 

[^expectationist-2]: Some theorists will put conditional probabilities in place of probabilities in this formula, so they'll use Pr(H\|X) rather than Pr(H). But since we're restricting attention to normal problems, this is a distinction without a difference.

I'm going to mostly follow suit. So why am I belabouring this point? One small reason and one large reason. The small reason is that one of the arguments I'll give concerning abnormal cases generalises to an argument for expectationism about normal cases. I go over the details of this in @sec-buchak. The other reason is dialectical.

Expectationism does a surprisingly bad job at matching untutored intuition about cases. In some important sense, it says that risk-aversion is irrational.[^expectationist-3] But intuition says that risk-aversion is rational. To be sure, expectationism does have things to say here. There is something like risk-aversion which makes sense given the declining marginal utility of money.[^expectationist-4] And, I think, expectationism can explain why risk-aversion seems rational to people who primarily think about bets in terms of goods with declining marginal value. But still, the expectationist is playing defence here. It seems intuitively like risk-aversion is rational, and as @Allais1953 showed, this implies that expectationism says unintuitive things about some fairly simple cases.

[^expectationist-3]: More precisely, it says the following. If B is between A and C in value, and the difference in value between A and B equals the difference in value between B and C, then a rational agent will be indifferent between getting B for sure, and a 50/50 chance of getting A or C. The risk-averse agent, in the sense of risk-aversion at issue here, will prefer B.

[^expectationist-4]: To modify the example of the previous footnote, if A, B and C are monetary rewards, and B is half-way between A and C in terms of its monetary value, then expectationism says that an agent can, and probably should, prefer B to a 50/50 chance of getting A or C.

Given that, it is surprising how many expectationists rely on intuitions about cases when assessing the merits of different theories of decision in abnormal cases. It often seems, when reading this part of the literature, that philosophical decision theorists endorse the following argument schema.

1.  The correct decision theory is the one that best tracks intuitions about cases.
2.  The decision theory that best tracks intuitions about cases is T (the preferred theory of the person making the argument).
3.  Therefore, the correct decision theory is T.

If the philosopher is an expectationist, they can't really endorse this argument. Premise 2 can't possibly be true. No matter how well theory theory does at tracking intuitions about abnormal cases, a modified version of their theory that allows for risk-aversion will do an even better job, especially when normal cases are considered. And for that reason, they can't really believe premise 1. After all, if premise 1 is true, then the correct decision theory for normal decision problems is not expectationist.

So this means that arguments like this one should not be used in decision theory. Of course, we can't entirely depart from intuitions about cases. If our theory disagrees too much with common sense it starts becoming a theory of something else [@Jackson1998, Ch. 2]. The role of intuitions is like the drawing on a wanted poster. It's not true that the criminal is the person who best resembles the drawing, but you should be very sceptical of a theory that the criminal looks nothing at all like the poster. Still, you should also be sceptical of a theory that the criminal is someone who lacks what we thought were necessary skills for committing the crime, even if the person who most resembles the poster lacks those skills. One aim of this book is to develop several plausible preconditions on a good theory of decision, most importantly the Exit Principle of @sec-indecisive, and argue that only GDT (or something like it) satisfies those preconditions. That's more philosophically significant than whether GDT best tracks intuitions about cases.

If expectationism does not maximise agreement with intuition, why is it so popular? It is because there are several plausible principles that are consistent with expectationism, but which are not consistent with the best alternative, namely the Quiggin-Buchak theory. These include[^expectationist-5]:

News is Valuable

:   It is never worse to have more information before making a decision, and it is typically better to have more information.

Sure Thing

:   If A is better than B conditional on *p* being true, and A is better than B conditional on *p* being false, then A is better than B.

Substitution of Indifferents

:   If Chooser is indifferent between A and B, then Chooser should be indifferent between any two gambles that have the same payouts in all but one possibility, and in that possibility one of them returns A, and the other returns B.

Exit Principle

:   If the choice Chooser makes does not make a difference to the payout Chooser gets if *p* is false, then it should not matter to Chooser whether they make their choice before or after finding out whether *p* is true.

[^expectationist-5]: Philosophers often attribute the result that expectationism implies News is Valuable to @Good1967, but it's really just a reformulation of a result due to David @Blackwell1951, which I think is a better attribution. That said, @Das2023 notes that a similar result is in an old note by C. S. @Peirce1876, first published in an academic setting in 1967, but (according to @Wible1994), in a US government report in 1879. So maybe the result is very old.

These four principles suggest four arguments for expectationism. First, argue that either expectationism is true, or the Quiggin-Buchak theory is true. Second, argue that one of these principles is a plausible second premise. Then conclude, since the principle rules out the Quiggin-Buchak theory, that expectationism must be true.

It's certainly not a requirement of any expectationist that they endorse all four of these arguments. It isn't even a requirement that they endorse any of these four; they could have some other argument. But since expectationism is not the intuitive theory, it is a requirement that they endorse some argument or other, probably something like one of these, to the conclusion that expectationism is true.

This is harder than it looks. EDT, for example, rejects both News is Valuable and Sure Thing in Newcomb's Problem. The news about what Demon has predicted is not, according to EDT, valuable. If Chooser learns what Demon predicted, they will (rationally) choose B, and get, in expectation, a worse return than if they had not learned this and chosen A. And choosing B is preferable conditional on either the Demon predicting, or not predicting, that Chooser will choose A. But A is preferable overall.

That said, it's not like all versions of CDT endorse these principles either. There are a lot of intuitive counterexamples to News is Valuable. It's good to avoid spoilers for movies or football matches. In asymmetric coordination games (such as @tbl-bach-stravinsky in @sec-gad), it's bad to have it be conventional wisdom that you know what the other person will do. You're sure not to get the best result that way. @Das2023 argues that even given expectationism, the argument for News is Valuable fails on externalist conceptions of evidence. There is more to say about each of these cases, but the problems for News is Valuable are substantial enough that it doesn't seem like a good premise in an argument against a decision theory.

And, perhaps more surprisingly, there are versions of CDT that reject Sure Thing. Dmitri @Gallownd argues that any version of CDT which is 'stable' in his sense will reject it. I suspect the argument he gives can generalise to some theories that are not stable in his sense as well. GDT avoids his argument only by the expedient of not offering a preference ordering over alternatives; it just says which choices are choice-worthy, not which choices should and should not be preferred to others.[^expectationist-6] So Sure Thing doesn't look like a safe starting point either, even if something like it might turn out to be true.

[^expectationist-6]: I'll have much more to say about this in @sec-select.

-----FINISH CHAPTER----

Expectationism has a big practical advantage; it lets us treat the payouts in a game table as expected values, not any kind of final value. This is useful because it is very rare that a decision problem results in outcomes that have anything like final value. Often we are thinking about decision problems where the payouts are in dollars, or some other currency. That's to say, we are often considering gambles whose payout is another gamble. Holding some currency is a bet against inflation; in general, the value of currency is typically highly uncertain.[^expectationist-7] For the expectationist, this is not a serious theoretical difficulty. As long as a dollar, or a euro, or a peso, has an expected value, we can sensibly talk about decision problems with payouts in those currencies. Depending on just how the non-expectationist thinks about compound gambles, they might have a much harder time handling even simple money bets.[^expectationist-8]

[^expectationist-7]: See @Alcoba2023 for what happens when people start thinking that bet is a bad one.

[^expectationist-8]: Joanna @Thoma2019 develops a subtle critique of some non-expectationist theories starting with something like this point.
