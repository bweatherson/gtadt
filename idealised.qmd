# Idealised {#sec-ideal}

## Introducing Ideal Theory {#sec-ideal-intro}

Game theorists, like philosophical decision theorists, are doing ideal theory. To see that they are doing ideal theory, compare what they say about two problems: **Salesman** and **Basketball**. The first is a version of what Julia Robinson dubbed the 'travelling salesman' problem.[^demons-and-decisions-1]

[^demons-and-decisions-1]: The dubbing is in @Robinson1949. For a thorough history of the problem, see @Schrijver2005. For an accessible history of the problem, which includes these references, see the wikipedia page on 'Traveling Salesman Problem'.

> **Salesman**
> Chooser is given the straight line distance between each pair of cities from the 257 represented on the map below. Using this information, Chooser has to find as short a path as possible that goes through all 257 cities and returns to the first one. The longer a path Chooser selects, the worse things will be for Chooser.

```{r salesman-points, fig.cap="The 257 cities that must be visited in the Salesman problem."}

theme_map <- function(base_size=9, base_family="") {
  theme_bw(base_size=base_size, base_family=base_family) %+replace%
    theme(axis.line=element_blank(),
          axis.text=element_blank(),
          axis.ticks=element_blank(),
          axis.title=element_blank(),
          panel.background=element_blank(),
          panel.border=element_blank(),
          panel.grid=element_blank(),
          panel.spacing=unit(0, "lines"),
          plot.background=element_blank(),
          legend.justification = c(0,0),
          legend.position = c(0,0)
    )
}

theme_set(theme_map())

all_states <- map_data("state") %>% 
  group_by(region) %>% 
  tally() %>% 
  select(state = region)

all_states$code <- c("AL", "AZ", "AR", "CA", "CO", "CT", "DE", "DC", "FL", "GA",
                     "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", "MA", 
                     "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", 
                     "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", 
                     "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY")

used_states <- 1:49

long_states <- all_states$state[used_states]
short_states <- all_states$code[used_states]

data("USCA312")
data("USCA312_GPS")

cities <- as_tibble(as.matrix(USCA312))

city_numbers <- tibble(
  id = 1:312,
  thecities = colnames(cities)
) %>% 
  mutate(used_city = case_when(str_sub(thecities, -2) %in% short_states  ~ 1,
                               TRUE ~ 0))

the_city_numbers <- filter(city_numbers, used_city == 1)$id


our_cities <- cities %>% 
  select(all_of(the_city_numbers)) %>% 
  slice(the_city_numbers)

our_gps <- USCA312_GPS %>% 
  slice(the_city_numbers) %>% 
  rowid_to_column()

city_matrix <- as.matrix(our_cities)

rownames(city_matrix) <- filter(city_numbers, used_city == 1)$thecities

## Fine tour
#tour_line <- solve_TSP(as.TSP(city_matrix), method="farthest_insertion")
#tour_line <- solve_TSP(as.TSP(city_matrix), method="two_opt", tour = tour_line)

## Not good tour
#tour_line <- solve_TSP(as.TSP(city_matrix), method="cheapest_insertion", start = 17) # - Very messy

## Generate tour by longitude - really bad
#tour_line <- TOUR(arrange(our_gps, long)$rowid, tsp = as.TSP(city_matrix))

## Best tour
load("tour_line.RData")

## Turn tour to map path
# paths <- tribble(
#   ~step, ~property, ~rowid, ~long, ~lat
# )
# 
# for (i in 1:nrow(our_gps)){
#   x <- tour_line[i]
#   first_city <- our_gps %>% slice(x)
#   next_city <- our_gps %>% slice(x %% 31)
#   paths <- paths %>% 
#     add_row(step = i, property = "from", rowid = first_city$rowid[1], long = first_city$long[1], lat = first_city$lat[1])# %>% 
#   #    add_row(step = i, property = "to", rowid = next_city$rowid[1], long = next_city$long[1], lat = next_city$lat[1])
# }
# 
# x <- tour_line[1]
# 
# paths <- paths %>% add_row(step = 24, property = "from", rowid = our_gps$rowid[x], long = our_gps$long[x], lat = our_gps$lat[x])


state_map_data <- map_data("state") %>%
  #  filter(subregion != "north" | is.na(subregion)) %>%
  filter(region %in% long_states) 

tour_map <- ggplot(state_map_data, aes(long, lat, group = group)) +
  geom_polygon(fill = "white", colour = "grey90") + 
  geom_point(data = our_gps %>% select(long, lat), aes(x = long, y = lat), size = 0.25, inherit.aes = FALSE) +
#  geom_path(data = paths %>% select(long, lat), aes(x = long, y = lat), inherit.aes = FALSE, colour = "grey30", alpha = 0.5 ) + 
  coord_quickmap()
#tour_length(tour_line)
tour_map

tour_map_points_only <- tour_map

#str_c(our_gps$name, sep = "; ", collapse = "; ")
```

Since there are 256! possible paths, and 256! â‰ˆ 10^727^, Chooser has a few options here.[^demons-and-decisions-2] Game theorists, and philosophical decision theorists, start with the assumption that the people in their models can solve these problems in zero time and at zero cost. That's not even approximately true for any actual person without technological assistance. Even with knowledge of the problem and a good computer, there are not that many actual people who you could properly model as being able to solve it in zero time and at zero cost.

[^demons-and-decisions-2]: The 256 cities are the cities in the lower 48 states from the 312 cities in North America that John Burkardt mapped in his dataset Cities, available at [people.sc.fsu.edu/\~jburkardt/datasets/cities/cities.html](https://people.sc.fsu.edu/~jburkardt/datasets/cities/cities.html). 

The question of how to think about people who do have to spend time and resources to solve a problem like this is an interesting one. We might call that problem one in *non-ideal decision theory*.[^demons-and-decisions-3] I won't say much about non-ideal decision theory in the body of this book, though I'll come back to it in @sec-nidt. What I mostly want to do now is use **Salesman** to say something about what the difference between ideal and non-ideal theory is. And that difference is brought up vividly by the following problem.

[^demons-and-decisions-3]: I'm borrowing the term 'non-ideal' from work in political philosophy. See @Valentini2012 for a good survey of some relevant work, and @Mills2005 for an important critique of the centrality of ideal theory in political philosophy. Critics of ideal theory, such as Mills, and @Sen2006, argue that we shouldn't base non-ideal theory on ideal theory. I'm going to agree, but my focus is primarily in the other direction. I'm going to argue that it isn't a constraint on ideal theory that it is useful in constructing a non-ideal theory.

> **Basketball**
> Chooser is at a casino, and a basketball game is about to start. Chooser knows that basketball games don't have draws or ties; one side will win. And Chooser knows the teams are equally balanced; each team is 50% likely to win. Chooser has three options. They can bet on the Home team to win, bet on the Away team to win, or Pass, and not bet. If they bet, they win \$100 if the team they bet on wins, and lose \$110 if that team loses. If they Pass, they neither gain nor lose anything.

Ideal decision theory says that in **Basketball**, Chooser should Pass. That's not the optimal outcome for Chooser. The optimal outcome is that they bet on the winning team. But since they don't know who that is, and either bet will, on average, lose them money, they should Pass rather than bet on Home or Away. We could have a theory that just evaluated the possible outcomes in any decision. I'll call this Outcome Evaluation Theory. Contrast this with two other theories. Game theory says that the ideal agent chooses the shortest route, whatever it is, in **Salesman** and does not bet in **Basketball**. If an ordinary reasonable person was advising a friend facing these two problems, they would give the same advice as the game theorist about **Basketball**, but in **Salesman** they would not simply say _Choose the shortest path!_, since that's useless advice. Rather they would suggest something about how to solve the problem, possibly by looking up strategies.

So we have three theories on the table: Outcome Evaluation Theory; the game theory approach, which I'll call Ideal Decision Theory; and the ordinary reasonable person approach, which I'll call Non-Ideal Decision Theory. We can distinguish these three theories by what they say to do in two examples introduced so far: Salesman and Basketball.

|     **Theory**     |    **Salesman**    | **Basketball** |
|:------------------:|:------------------:|:--------------:|
| Outcome Evaluation |   Shortest route   | Bet on winner  |
|   Ideal Decision   |   Shortest route   |      Pass      |
| Non-Ideal Decision | Study optimization |      Pass      |

: How three kinds of theories handle two problems. {#tab-three-theories}

Game theory agrees with the middle row. GDT, the theory I'm developing in this book, does so too. And so do almost all decision theorists working in philosophy.^[The exceptions are people working in 'descriptive decision theory' [@ChandlerSEP]. But that's normally not taken to be a normative theory in any respect; it isn't about what people should do in problems like **Salesman**, but what they actually do.] So in trying to convince philosophers to adopt GDT, I'm not asking them to change their view on this point. But still, this is odd. What is the benefit of a theory of decision that does not produce the best outcomes, and does not produce useful, reasonable advice?

We could say that if Chooser were ideal, they would agree with Ideal Decision Theory. But why we should care about what would have if Chooser were ideal, since Chooser is not in fact ideal? One might think that knowing what the ideal is gives Chooser something to aim for. Even if Chooser is not ideal, they can try to be closer to the ideal. The problem is that trying to be more like the ideal will make things worse. The ideal agent will announce the best answer they have after spending no time calculating the solution to **Salesman**, and resembling the ideal agent in that respect will make Chooser worse.[^idealised-1] And there is a separate problem. Why say it is ideal to make a choice in **Basketball** that Chooser knows will lead to a sub-optimal outcome? We can make progress on both these problems, what it means to say something is ideal, and why we should care about the ideal, but stepping back and asking what we even mean by 'ideal', and 'idealisation'.

[^idealised-1]: This is a special case of Lipsey and Lancaster's Theory of the Second Best [@LipseyLancaster]. If you don't have control over every parameter, setting the parameters you do control to the ideal values is generally inadvisable.

In philosophy, it turns out we have two very different uses of the term 'idealisation'. One is the kind of idealisation we see in, for example, Ideal Observer theories in ethics. The other is the kind of idealisation we see in, for example, Ideal Gas models in chemistry. It's important to not confuse the two. Think about the volumeless, infinitely dense, molecules in an Ideal Gas model. To say that this is an idealised model is not to say that having volume, taking up space, is an imperfection. The point is not to tell molecules what the perfect size is. ("The only good molecule is a volumeless molecule.") Nor is it to tell them that they should approximate the ideal. ("Smaller the better, fellas.") It's to say that for some predictive and explanatory purposes, molecules behave no differently to how they would behave if they took up no space.[^idealised-2]

[^idealised-2]: I'm drawing here on work on the nature of idealisations by Michael @Strevens2008 and by Kevin @Davey2011.

The best way to understand game theorists, and most philosophical decision theorists, is that they are using idealisations in this latter sense. The ideal choosers of decision theory are not like the Ideal Observers in ethics, but like the Ideal Gases. The point of the theory is to say how things go in a simplified version of the case, and then argue that this is useful for predictive and explanatory purposes because, at least some of the time, the simplifications don't make a difference.

## Uses of Ideal Theory {#sec-uses-ideal}

So 

One nice example of this working is George Akerlof's discussion of the used car market @Akerlof1970. In the twentieth century, it was common for lightly used cars to sell at a massive discount to new cars. There was no good explanation for this, and it was often put down to a brute preference for new cars. What Akerlof showed was that a model where (a) new cars varied substantially in quality, and (b) in the used car market, buyers had less information about the car than sellers, you could get a discount similar to what you saw in real life even if the buyers had no special preference for new cars. Rather, buyers had a preference for good cars, and took the fact that this car was for sale to be evidence that it was badly made. It was important for Akerlof's explanatory purposes that he could show that people were being rational, and this required that he have a decision theory that they followed. In fact what he used was something like GDT. We now have excellent evidence that something like his model was correct. As the variation in quality of new cars has declined, and the information available to buyers of used cars has risen, the used car discount has just about vanished. (In fact it went negative during the pandemic, for reasons I don't at all understand.)

I'll end this section with a response to one objection, one caveat, and one surprising bonus to doing idealised decision theory this way.

The objection is that decision theory isn't actually that helpful for prediction and explanation. If all that it says are things like when rain is more probable, more people take umbrellas, that doesn't need a whole academic discipline. The objector might echo the sentiments that Keynes expresses in a widely quoted passage. (Though it is rare that the key quote is put in its proper context; I think I'm not guilty of that here.)

> But this _long run_ is a misleading guide to current affairs. _In the long run we are all dead_. Economists set themselves too easy, too useless a task if in tempestuous seasons they can only tell us that when the storm is long past the ocean will be flat again. [@Keynes1923, 80, emphasis in original]

Donâ€™t focus on the temporal connotations of Keynesâ€™s terminology of â€˜long runâ€™. Whatâ€™s characteristic of his long run is not that it takes place in the distant future. What is characteristic of it instead is that it takes place in a world where some sources of interference are absent. Itâ€™s a world where we sail but there are no storms. Itâ€™s a study where we abstract away from storms and other unfortunate complications. And thatâ€™s whatâ€™s characteristic of Ideal Decision Theory. We know that people cannot easily solve hard arithmetic problems, but we abstract away from that fact. Does this leave the resulting theory "easy and useless"?

To see that it's not "easy", it simply suffices to take a casual glance at any economics journal. But what about Keynes's suggestion that it is "useless"? If all we could say was that when it's more likely to rain, more people will take umbrellas, that objection would start to have some bite. But we can say much more. In non-cooperative games, the predictions, and explanations, the theory delivers can be rather surprising. One nice case of this is the discussion of Gulf of Mexico oil leases in @Wilson1967.[^idealised-3] But here's a simpler surprising prediction that you need something like GDT to get.[^idealised-4]

[^idealised-3]: I learned about this paper from the excellent discussion of the case in @Sutton2000.

[^idealised-4]: A somewhat similar point is made in the example of the drowning dog on page 216 of @Bonanno2018.

Imagine Row and Column are playing rock-paper-scissors. A bystander, C, says that he really likes seeing rock beat scissors, so he will pay whoever wins by playing rock \$1. Assuming that Row and Column have no ability to collude, the effect of this will be to shift the payouts in the game they are playing from left table to right table, where *c* is the value of the dollar compared to the value of winning the game. This changes the game they are playing from @tbl-rps-basic to @tbl-rps-modified.

::: {#tbl-rps layout-ncol="2"}
|              |          |           |              |
|:------------:|:--------:|:---------:|:------------:|
|              | **Rock** | **Paper** | **Scissors** |
|   **Rock**   |   0,0    |   -1,1    |     1,-1     |
|  **Paper**   |   1,-1   |    0,0    |     -1,1     |
| **Scissors** |   -1,1   |   1,-1    |     0,0      |

: Original game {#tbl-rps-basic}

|              |          |           |              |
|:------------:|:--------:|:---------:|:------------:|
|              | **Rock** | **Paper** | **Scissors** |
|   **Rock**   |   0,0    |   -1,1    |   1+*c*,-1   |
|  **Paper**   |   1,-1   |    0,0    |     -1,1     |
| **Scissors** | -1,1+*c* |   1,-1    |     0,0      |

: Modified game {#tbl-rps-modified}

Two versions of Rock-Paper-Scissors
:::

The surprising prediction is that this will *decrease* the frequency with which the bystander gets their way. The incentive will not make either party play rock more often, they will still play it one third of the time, but the frequency of scissors will decrease, so the *rock smash* outcome will be less frequent. Moreover, the bigger the incentive, the larger this increase will be[^idealised-5]. Simple rules like "When behaviour is rewarded, it happens more often" don't always work in strategic settings, and it takes some care to tell when they do work.

[^idealised-5]: The proof is in @sec-rps.

The caveat is that there is a reason that this particular idealisation is chosen, at least as the first attempt. There are a lot of stylised facts about people that we could use in a model of behaviour. In game theory we concentrate on the ways in which people are, at least approximately much of the time, somewhat rational. People who prefer vanilla to chocolate really do buy vanilla more than chocolate. We could also choose stylised facts that are not particularly rational. But there is a worry that these will not remain facts, even approximately, when the stakes go up. And for some purposes, what people do in high stakes situations might be really important. If you think that people are more careful in high stakes situations, and that this care translates into more rational action, and it's particularly important to make the right predictions in high stakes cases, it makes sense to focus on idealisations that are also true of perfectly rational people.

There is a tricky complication here that isn't always attended to in theory, though in practice it's less of a problem. Especially in decision theory, we idealise away from computational shortcomings, but not away from informational shortcomings. We take the chooser's information as fixed, and ask how they'll decide. In high stakes cases, people don't just get better calculators, they get more information. If this is why we idealise, why don't we idealise away from ignorance? The reason is that in a lot of cases, either it is impossible to get the evidence the chooser needs, because it is about the future, or it is challenging because there is someone else working just as hard to prevent the chooser getting the information. It's not a coincidence that game theory, and decision theory, are most explanatory when the chooser's ignorance is about the future, or about something that someone is trying to hide from them.

There is one surprising bonus from starting with these rational idealisations. Sometimes one gets a powerful kind of explanation from very carefully working out the ideal theory, and then relaxing one of the components. At a very high level of abstraction, that's what happened with the development of cursed equilibrium [@EysterRabin2005]. The explanations one gets these ways are, to my mind, very surprising. The models have people acting as if they have solved very complex equations, but have ignored simple facts, notably that other people may know more than they do. But if the model fits the data, it is worth taking seriously. And while it was logically possible to develop a model like cursed equilibrium without first developing an ideal model and then relaxing it, it seems not surprising that in fact that's how the model was developed.

So our topic is idealised decision theory. In practice, that means the following things. The chooser can distinguish any two possibilities that are relevant to their decision, there is no unawareness in that sense, and they know when two propositions are necessarily equivalent. They can perform any calculation necessary to making their decision at zero cost. They have perfect recall. They don't incur deliberation costs; in particular, thinking about the downsides of an option does not reduce the utility of ultimately taking that option, as it does for many humans. They know what options they can perform, and what options they can't perform. I'll argue in @sec-mixed that it means they can play mixed strategies. Finally, I'll assume it means they have numerical credences and utilities. I'm not sure this should be part of the same idealisation, but it simplifies the discussion, and it is arguable that non-numerical credences and utilities come from the same kind of unawareness that we're assuming away. [@GrantEtAl2021]

So the problems our choosers face look like this. There are some possible states of the world, and possible choices. The chooser knows the value to them of each state-choice pair. (In @sec-expect I'll say more about this value.) The states are, and are known to be, causally independent of the choices. But the states might not be probabilistically independent of the choices. Instead, we'll assume that the chooser has a (reasonable) value for Pr(*s* \| *c*), where *s* is any one of the states, and *c* is any one of the choices. The question is what they will do, given all this information.

And the point of answering this question is not to give people advice, which would often be harmful advice given that they do not satisfy the idealising assumptions, but to predict and explain their behavior. David Lewis gives a similar account of the purpose of decision theory in a letter to Hugh Mellor. The context of the letter, like the context of this section, is a discussion of why idealisations are useful in decision theory. Lewis writes,

> We're describing (one aspect of) what an ideally rational agent would do, and remarking that somehow we manage to approximate this, and perhaps -- I'd play this down -- advising people to approximate it a bit better if they can. [@Lewis1981Mellor, 432]

To conclude this chapter on an historical note, I want to compare the view I'm adopting to the position Frank Knight puts forward in this famous footnote.

> It is evident that the rational thing to do is to be irrational, where deliberation and estimation cost more than they are worth. That this is very often true, and that men still oftener (perhaps) behave as if it were, does not vitiate economic reasoning to the extent that might be supposed. For these irrationalities (whether rational or irrational!) tend to offset each other. The applicability of the general "theory" of conduct to a particular individual in a particular case is likely to give results bordering on the grotesque, but *en masse* and in the long run it is not so. The *market* behaves *as if* men were wont to calculate with the utmost precision in making their choices. We live largely, of necessity, by rule and blindly; but the results approximate rationality fairly well on an average. [@Knight1921 67n1]

Like Knight, I think that explanations in social science can treat people as rational even if they are not, even if it would "give results bordering on the grotesque" to imagine them as perfectly rational. That's because, at least in the right circumstances, the irrationalities are irrelevant, or they cancel out, and the "as if" explanation goes through. Now I do disagree with the somewhat blithe attitude Knight takes towards the possibility that these imperfections will not cancel out, that they will in fact reinforce each other and be of central importance in explaining various phenomena. But that's something to be worked out on a case-by-case basis. We should not presuppose in advance either that the imperfections be irrelevant or that they will be decisive.

There is one other point of agreement with Knight that I want to emphasise. If we don't act by first drawing Marshallian curves and solving optimisation problems, how do we act? As he says, we typically act "by rule". Our lives are governed, on day-by-day, minute-by-minute basis, by a series of rules we have internalised for how to act in various situations. The rules will typically have some kind of hierarchical structure - do this in this situation unless a particular exception arises, in which case do this other thing, unless of course a further exception arises, in which case, and so on. And the benefit of adopting rules with this structure is that they, typically, produce the best trade off between results and cognitive effort.

One other useful role for idealised decision theory is in the testing and generation of these rules. We don't expect people who have to make split-second decisions to calculate expected utilities. But we can expect them to learn some simple heuristics, and we can expect theorists to use ideal decision theory to test whether those heuristics are right, or whether some other simple heuristic would be better. This kind of approach is very useful in sports, where athletes have to make decisions very fast, and there is enough repetition for theorists to calculate expected utilities with some precision. But it can be used in other parts of life, and it is a useful role for idealised decision theory alongside its roles in prediction and explanation.
