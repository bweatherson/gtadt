# Dual Mandate {#sec-dualmandate}

## Decision Trees and How to Climb Them

Say a decision tree is a series of steps with the following characteristics.

-   At every step, Chooser either receives some information, or makes a choice.
-   Chooser knows before the first step what possible choices will be available at each step, given the prior steps, or what possible pieces of information could be received.
-   No matter what happens, the tree ends after finitely many steps. (Though it may end after more or fewer steps depending on what happens).
-   Chooser knows before the first step what payout they will receive given each possible sequence of choices and information.
-   Before the first step, chooser has a probability for each possible piece of information they could receive, given the prior steps in the tree.

That's incredibly abstract, but it excludes some possibilities. It excludes cases where Chooser learns along the way that they have hitherto unknown abilities. It excludes cases where Chooser gains the capacity to think new relevant thoughts along the way, say by meeting a new person and gaining the capacity to have singular thoughts about them.[^dual-old-1] Still, it does cover a lot of cases.

[^dual-old-1]: Following @Stalnaker2008, I think it excludes the Sleeping Beauty case, since there Beauty gains the capacity to have singular thoughts about a time, the 'now' when she awakes, that she did not previously have.

Say a strategy for a decision tree is a plan for what to do in every possible choice situation. Following the game theory textbooks, I really do mean *every* here. A strategy should say what to do in cases that are ruled out by Chooser's prior choices. A strategy for playing chess as White might say to start with e4, but also include plans for what to do if you inexplicably start Na3. There are both mathematical and philosophical reasons for having such an expansive conception of strategies, and I'll come back to one reason why later in this chapter.

Here are two classes of philosophical views about how decision makers facing such trees should act.

One approach is what I'll call the **purely strategic** approach. This says that Chooser should treat the problem as one of making a one-off choice between the (potentially very large) number of strategies. Then at each step, an act is permissible iff it is part of the strategy Chooser has permissibly chosen. On this view, the only difference between a static choice, where Chooser just makes a decision, finds out something about the world, and gets a return, and a dynamic choice, like when Chooser faces a tree like this, is that in the latter Chooser has to implement their choice in stages.

Another approach is what I'll call the **purely consequentialist** approach. This says that at each time Chooser has to make a decision, they are facing a wholly new decision problem, and the only rational constraints they face are ones that they would be facing if they were at the start of a tree. Moreover, Chooser knows now that their future self will be like this. So when they now make decisions whose consequences turn on (among other things) what their future self will do, they regard those decisions by their future self as matters for prediction, not planning. They take the same attitude towards their future self as they take to other decision makers. That means that they may assume (if it's given as an assumption) that they will be rational, and if there are multiple possible rational choices, they can have a probability distribution over the choices the person may make, but they don't regard themselves as in a position to bind their future self.

I'm going to reject both of these approaches. My preferred approach follows what I take the orthodoxy in game theory to be. This view follows the lead suggested by @Selten1965 which says in effect that both the strategic and consequentialist approaches are correct. But because they are both correct, it can't be right to have a *purely* strategic, or a *purely* consequentialist view. I'm going to call this the **dual mandate** approach. More formally, it says that in such a tree, every choice Chooser makes should be defensible on consequentialist grounds, and the set of choices that Chooser makes should all be part of some strategy that is defensible on strategic grounds. Chooser's choices must make sense taken one-at-a-time, and they must make sense taken collectively. As I said, I think this is more or less game-theoretic orthodoxy, but I don't think it's a common position in the philosophical literature.

The three options from the last three paragraphs do not exhaust the field. There are views that reject the way I have set up the problem. In the first bullet point, I said that Chooser might either receive information, or make a choice. In a game, there are two kinds of information Chooser might receive. Either a genuinely random possibility might get resolved some way, or a rational player might act.[^dual-old-2] One could have a theory that treated the two possibilities differently. For instance, I can see the appeal of having a view that was more strategic when it came to interactions with others, but more consequentialist when it came to interactions with purely random parts of the world.[^dual-old-3] My arguments below against the purely strategic approach are meant to extend to these moderately strategic approaches, but it's impossible to predict every possible move that might be made, and I suspect there are some interesting approaches on these lines that aren't ruled out, even if everything I say here is true.

[^dual-old-2]: Game theorists typically refer to the first as 'Nature' making a move.

[^dual-old-3]: For people familiar with the decision theory literature, this kind of position may be appealing to those whose intuitions supported taking one box in Newcomb's Problem, but supported taking the causally dominant choice in "Medical Newcomb Problems" (to borrow a phrase from Huw @Price1991).

In philosophy, the purely strategic approach is often called the resolute approach[^dual-old-4], and the purely consequentialist approach is called the sophisticated approach. But I don't think it's too bad to introduce new names here. My use of consequentialist follows the very influential use of @Hammond1988, and it's useful to connect the strategic approach to game-theoretic strategies.

[^dual-old-4]: Most notably defended by @McClennan1990.

The orthodoxy in game theory, going back to at least @Selten1965, is that both views are correct. When faced with a decision tree, Chooser should follow the advice of the sophisticated theorists, and (given they are ideally rational) do what would be best on the assumption that future choices will be rational. But in doing so, they should instantiate (part of) a strategy that could be rationally chosen by the resolute chooser. I call this the Dual Mandate approach, and I am going to defend it.

## Against Purely Strategic Approaches

Game theorists usually reject the purely strategic approach because it means sometimes making a decision that one knows will have worse consequences than an available alternative. I'm going to go over three strategic decisions in this section which illustrate this in the three subsections. The first is a textbook example of why game theorists reject purely strategic versions. (Quite literally, it's from a textbook.) This isn't really enough to make the philosophical point, however, since many philosophers know of such examples, and think that either (a) the option game theorists rule out is desirable, so this is not actually a counterexample to purely strategic approaches, or (b) the example stops working if we add that weakly dominated strategies are not choice-worthy.[^dual-old-5] So in following two sub-sections I'll go over distinct examples that motivate rejecting each position.

[^dual-old-5]: To the best of my knowledge, everyone who makes complaint (a) is a one-boxer in [Newcomb's Problem](intro.qmd#tbl-newcomb), and everyone who makes complaint (b) is a two-boxer. This might help in placing the arguments in what follows.

### The Textbook Counterexample

The example that Bonanno uses to argue against purely strategic approaches is a one-shot version of Reinhard Selten's Chain Store Paradox [@Selten1978].

> An industry is currently a monopoly and the incumbent monopolist is making a profit of \$5 million. A potential entrant is considering whether or not to enter this industry.\
> - If she does not enter, she will make \$1 million in an alternative investment. - If she does enter, then the incumbent can either fight entry with a price war whose outcome is that both firms make zero profits, or it can accommodate entry, by sharing the market with the entrant, in which case both firms make a profit of \$2 million. [@Bonanno2018, 86]

The tree diagram of the game is in @fig-chain-store. I assume here that a million dollars is worth 1 util to the potential entrant and the incumbent. And I'll list the incumbent's payouts first, then the potential entrant's. Finally, and a little implausibly, assume that there are no reputational or strategic benefits to either player, so the millions in profit are all that matter.

```{r engine='tikz'}
#| label: fig-chain-store
#| fig.cap: "Tree Diagram of the Chain Store Game"
#| fig.ext: 'png'
#| cache: TRUE
#| echo: FALSE
#| fig.width: 4
\usetikzlibrary{calc}

\begin{tikzpicture}[scale=1.4,font=\footnotesize]
\tikzset{
% Two node styles for game trees: solid and hollow
solid node/.style={circle,draw,inner sep=1.5,fill=black},
hollow node/.style={circle,draw,inner sep=1.5},
square node/.style={rectangle,draw, inner sep = 1, fill = black}
}

% Specify spacing for each level of the tree
\tikzstyle{level 1}=[level distance=20mm,sibling distance=20mm]
\tikzstyle{level 2}=[level distance=20mm,sibling distance=20mm]
\tikzstyle{level 3}=[level distance=20mm,sibling distance=20mm]

% The Tree
\node(0)[hollow node,label=above:{Entrant}]{}
    child[grow=down]{
        node[square node,label=below:{5, 1}]{} edge from parent node [left]{Out}}
     child[grow=right]{node(1)[solid node, label=above:{Incumbent}]{}
        child[grow=down]{node[square node,label=below:{0, 0}]{} edge from parent node [left]{Fight}}
        child[grow=right]{node[square node,label=right:{2, 2}]{} edge from parent node [below]{Accommodate}}
            edge from parent node [below]{In}
            }
;
\end{tikzpicture}
```

The orthodox (and I think correct) treatment of the game starts with Incumbent's potential choice. They have a choice between 2 for sure and 0 for sure, and 2 is larger than 0. So if the game gets to that point, they will Accommodate. Entrant can figure this out, and hence knows that if are In they too will get a payout of 2, while they will get a payout of 1 if they are Out. Since 2 is greater than 1, they will be In. So the game ends with Entrant In, and Incumbent Accommodating.

But look at the strategic form of the game, in @tbl-chain-store. This is quite a simple structure, since each player only has one possible choice. I'll write the strategy table with Incumbent choosing a row, and Entrant choosing a Column.

|                 |  In  | Out  |
|:---------------:|:----:|:----:|
| **Accommodate** | 2, 2 | 5, 1 |
|    **Fight**    | 0, 0 | 5, 1 |

: The strategic form of the Chain Store Game {#tbl-chain-store}

The first thing to note is that the pair Out, Fight is a Nash equilibrium of the game. In decision theoretic terms, Fight is self-ratifying if Incumbent thinks that Entrant is a perfectly reliable Demon who will maximise profits given their (correct!) prediction about what Incumbent will do. And not only is it ratifying, it returns more than the unique backward induction solution to the game.

Putting the last few paragraphs together, we get the following conclusions. In the extensive form of the game, where Incumbent chooses after Entrant's move is revealed, there is only one rational move: Accommodate. But in the strategic form of the game, Fight is at least rationally permissible, and perhaps rationally desirable. So a strategy that could be rationally chosen in the strategic form of the game, namely Fight, cannot be rationally chosen in the extensive form. So purely strategic approach to decisions in games is wrong; it says that Fight is rationally permissible in @fig-chain-store but it is not.

There are two natural ways to object to the reasoning in the last paragraph; one could reject the claim that it's wrong to Fight at stage two, or one could reject the claim that it's acceptable to choose the strategy Fight in the strategic form. The next two sections will look at these options in turn. I'm going to argue neither gives us a reason to stick with a purely strategic theory.

### Strategic EDT {#sec-sedt}

Call Strategic EDT (hereafter, SEDT) the theory that a rational Chooser will approach decision trees by (a) evaluating each possible strategy using EDT, (b) choosing one of the strategies that has maximal value, and (c) acting at each stage by implementing the chosen strategy, even if it no longer looks optimal. Ben Levinstein and Nate Soares [@LevinsteinSoares2020] have a theory that is a bit more subtle than SEDT, but (if I understand the view correctly), agrees with SEDT on all the cases I'll discuss in this subsection.

One way to motivate SEDT is by thinking about a version of [Newcomb's Problem](index.qmd#tbl-newcomb) where Demon's predictions are revealed to Chooser. The game tree for this variant on Newcomb's Problem is in @fig-open-newcomb.

```{r engine='tikz'}
#| label: fig-open-newcomb
#| fig.cap: "Tree Diagram of the Open Newcomb game."
#| fig.ext: 'png'
#| cache: TRUE
#| echo: FALSE
#| fig.width: 4
\usetikzlibrary{calc}

\begin{tikzpicture}[scale=1.4,font=\footnotesize]
\tikzset{
% Two node styles for game trees: solid and hollow
solid node/.style={circle,draw,inner sep=1.5,fill=black},
hollow node/.style={circle,draw,inner sep=1.5},
square node/.style={rectangle,draw, inner sep = 1, fill = black}
}

% Specify spacing for each level of the tree
\tikzstyle{level 1}=[level distance=10mm,sibling distance=20mm]
\tikzstyle{level 2}=[level distance=10mm,sibling distance=10mm]
\tikzstyle{level 3}=[level distance=20mm,sibling distance=20mm]

% The Tree
  \node[hollow node,label=above:{Demon}]{}
    child {
      node[solid node,label=left:{Chooser}]{}
      child {
        node[square node,label=below:{1000}]{}
        edge from parent node[left] {A}
      }
      child {
        node[square node,label=below:{1001}]{}
        edge from parent node[right] {B}
      }
      edge from parent node[left] {PA}
    }
    child {
      node[solid node,label=right:{Chooser}]{}
      child {
        node[square node,label=below:{0}]{}
        edge from parent node[left] {A}
      }
      child {
        node[square node,label=below:{1}]{}
        edge from parent node[right] {B}
      }
      edge from parent node[right] {PB}
    };
\end{tikzpicture}
```

In @fig-open-newcomb, regular EDT says to choose B. After all, no matter which node one is at, the best thing to do given that one is there is to take the extra 1 on offer. But it's surprisingly hard to come up with reasons to choose A in the original problem that do not extend to this problem. The main reason which is offered, that people who choose A end up richer, applies equally well here. And the main objection that opponents make, that choosing A is strange because one knows that one would choose B once one discovered what prediction was made, does not have any bite if one would follow SEDT and not in fact choose B were the prediction revealed.

It's a little amusing to imagine a proponent of regular EDT playing a version of @fig-open-newcomb. Presumably they would do whatever they could to not learn what the prediction was until they could make a choice. After all, by their own lights, learning what the prediction was would cost them, in expectation, 999. So depending on how this information will be revealed, they will close their eyes, sing "La la la I can't hear you" to block out noises, maybe hold their breath if the information will be revealed by distinctive smells, and so on. This doesn't seem like an image of a practically rational Chooser, and SEDT avoids all these problems.

That's not to say that SEDT is entirely without its own intuitive costs, even in simple cases like @fig-open-newcomb. The person carrying out SEDT will give the following speech. "I am not here to be a hero; I'm not following some philosophical theory just because it's cool. My job is making money. And, now that the prediction is revealed, I can see that I'll make more money choosing B. But I'm taking A." Again, this doesn't sound great. But, they argue, it does in fact lead to getting more money, on average, so maybe there is something to it. And SEDT does not agree with EDT in the cases I described in @sec-war-on-war, so I can't say that they are relying, as EDT relies, on too narrow a diet of cases.

Still, SEDT is not defensible in all somewhat realistic situations. The example I'll use to demonstrate this is rather more violent than the other examples in this book. But it needs to be rather violent in order to rule out the possibility of there being strategic or reputational considerations that are being left out.

Chooser is the Prime Minister of a small country, and they are threatened by a large nearby country, Neighbour. Unfortunately, Neighbour is thinking of carpet bombing Chooser's capital, in retaliation for some perceived slight during trade negotiations. Chooser has no air defences to stop the bombing, and no allies who will rally to help.

Fortunately, Chooser has a mighty weapon, a Doomsday device, that could destroy Neighbour. Chooser has obviously threatened to use this, but Neighbour suspects it is a bluff. This is for a good reason; the doomsday device would also destroy Chooser's own country. Neighbour is known to employ Demon who is at least 99% accurate in predicting what military plans Chooser will take.[^dual-old-6] In practice, all Demon has to do is predict So Chooser can do Nothing (N), or use the Doomsday device (D), should neighbour attack. Chooser would obviously prefer no attack, and would certainly not use the device preemptively. And Neighbour will attack iff Demon predicts that Chooser will do Nothing. Given all that the decision table that Chooser faces is in @tbl-retaliation.

[^dual-old-6]: It's very important to this example that Demon is not perfectly accurate. There hasn't been as much attention as there might have been to what happens to theories like SEDT in the context of good but not perfect Demons.

    One might worry that the case is not, as promised, realistic, because states do not in fact have Demons. That's true, but they do have spies, and analysts, and they are somewhat reliable in making predictions. It seems plausible that they could be reliable enough to get the case to work.

|       | PN  | PD  |
|:-----:|:---:|:---:|
| **N** | -1  |  0  |
| **D** | -50 |  0  |

: Deciding whether to retaliate. {#tbl-retaliation}

In the top left, Neighbour bombs Chooser's capital, thinking correctly that Chooser will not retaliate. In the top right and lower right, neighbour is sufficiently scared of the doomsday device that they do nothing. But in the bottom left, Neighbour attacks, and Chooser retaliates, creating a disaster for everyone, something 50 times worse than even the horrors of the carpet bombing.

Still, if Chooser is picking a strategy before anything starts, the strategy with the highest value, according to EDT, is to plan to use the Doomsday device. This has an expected return of -0.5; since one time in a hundred it returns -50, and otherwise it returns 0. That's what SEDT says one should do. And it says Chooser should quite literally stick to their guns, even if they see the bombers coming, and they realise their bluff has failed.

This seems absurd to me, and it is the kind of result that drives game theorists to the dual mandate. In case that example isn't decisive enough, let's consider two more variants on it.

Change the example so that Chooser has two advisors who are talking to them as the bombers come in. One of them says that the Demon is 99% reliable. The other says that the Demon is 97% reliable. Whether Chooser launches the Doomsday device should, according to SEDT, depend on which advisor Chooser believes. This is just absurd. A debate about the general accuracy of a Demon can't possibly be what these grave military decisions are based on.

Change the example again, and make it a bit more realistic. Chooser has the same two advisors, with the same views. Chooser thinks the one who says the Demon is 99% reliable is 60% likely to be right, and the other 40% likely. So Chooser forms the plan use the Doomsday device, because right now that's the strategy with highest expected return. But having made that decision, much to everyone's surprise, Neighbour attacks. SEDT now says to launch the Doomsday device.

But think about how the choice of plans looks to Chooser now. The actions of Neighbour are evidence about the reliability of Demon. And a simple application of Bayes' Rule says that Chooser should now think the advisor who thought the demon was 97% reliable is 2/3 likely to be right. That is, given Chooser's current evidence merely about the Demon's reliability (and not about what the Demon actually did), SEDT says not to use the Doomsday device. Yet despite it not being either the utility maximising strategy, or the utility maximising choice, SEDT says to launch the Doomsday device. This seems completely absurd, and enough to have us move to a new theory.

#### Actions and Strategies {#sec-non-equiv}

In . @fig-stalnaker-centipede is the game tree (in the sense described in @sec-dualmandate) for one such example.

```{r engine='tikz'}
#| label: fig-stalnaker-centipede
#| fig.cap: "Tree Diagram of the Centipede Game"
#| fig.ext: 'png'
#| cache: TRUE
#| echo: FALSE
#| fig.width: 4
\usetikzlibrary{calc}

\begin{tikzpicture}[scale=1.4,font=\footnotesize]
\tikzset{
% Two node styles for game trees: solid and hollow
solid node/.style={circle,draw,inner sep=1.5,fill=black},
hollow node/.style={circle,draw,inner sep=1.5},
square node/.style={rectangle,draw, inner sep = 1, fill = black}
}

% Specify spacing for each level of the tree
\tikzstyle{level 1}=[level distance=20mm,sibling distance=20mm]
\tikzstyle{level 2}=[level distance=20mm,sibling distance=20mm]
\tikzstyle{level 3}=[level distance=20mm,sibling distance=20mm]

% The Tree
\node(0)[hollow node,label=above:{Chooser}]{}
    child[grow=down]{
        node[square node,label=below:{$2$}]{} edge from parent node [left]{$D_1$}}
     child[grow=right]{node(1)[solid node, label=above:{Demon}]{}
        child[grow=down]{node[square node,label=below:{$1$}]{} edge from parent node [left]{$d$}}
        child[grow=right]{
            node(2)[solid node, label=above:{Chooser}]{}
                    child[grow=down]{node[square node,label=below:{$0$}]{} edge from parent node [left]{$D_2$}}
                    child[grow=right]{node[square node,label=right:{$3$}]{} edge from parent node [below]{$A_2$}}                    
                    edge from parent node [below]{$a$}
                    }
            edge from parent node [below]{$A_1$}
            }
;
\end{tikzpicture}
```

There are three stages, though the game might end at any stage. Chooser is the mover at stage 1 and, if the game gets that far, stage 3. Demon is the mover at stage 2, again if the game gets that far. At stage 1 and 2, if the mover moves down, the game ends. After stage 3, the game ends either way. Chooser's payouts are given on the tree. Demon's disposition is to do whatever they predict Chooser will do at stage 3, and they are arbitrarily good at predicting Chooser's strategy.[^dual-old-7]

[^dual-old-7]: This problem is very closely modelled on a game described by Stalnaker [-@Stalnaker1998, 47]. But note that how I've described Demon is different to how Stalnaker describes 'Bob', the player who moves at stage 2 in his version of the game. That's why I get what appears to be a different analysis to Stalnaker; we're not disagreeing here I think, just analysing different games.

In this dynamic game, the only sensible thing for Chooser to do at stage 1 is to play A~1~. That's because they know that if they get to stage 3, they will play A~2~, getting 3, rather than D~2~, getting 0.[^dual-old-8] So they should believe that Demon will predict that they will play A~2~, and hence will play a. So they should believe at stage 1 that playing A~1~ will get 3, while playing D~1~ will get 2, so they should play A~1~ at stage 1.

[^dual-old-8]: I'm assuming here that Chooser knows that they will be rational in the future, and this knowledge persists no matter what earlier choices they make. This is a substantial idealisation, but makes sense given the other idealisations that were described in @sec-idealised.

None of that should be too surprising; it's the standard backward induction solution of the game, though we've had to be a bit careful in just how we describe Demon for it to go through. But now consider what happens in the strategic form of the game. Chooser has four strategies: A~1~ or D~1~, crossed with A~2~ or D~2~. Let's simply give these strategies names, as follows.

| Strategy | Move 1 | Move 2 |
|:--------:|:------:|:------:|
|   S~1~   |  D~1~  |  D~2~  |
|   S~2~   |  D~1~  |  A~2~  |
|   S~3~   |  A~1~  |  D~2~  |
|   S~4~   |  A~1~  |  A~2~  |

: The possible strategies for Chooser in @fig-stalnaker-centipede. {#tbl-stalnaker-centipede-strategies}

Then Demon has two moves as well, which I'll call PO and PE. PO means that Demon predicts that Chooser will play an odd numbered strategy, i.e., S~1~ or S~3~. That is, Demon predicts that Chooser will play (or be disposed to play) D~2~. So PO is equivalent to Demon playing (or being disposed to play) d. PE means that Demon predicts that Chooser will play an even numbered strategy, i.e., S~2~ or S~4~. That is, Demon predicts that Chooser will play (or be disposed to play) A~2~. So PO is equivalent to Demon playing (or being disposed to play) a. Given that, we can describe the strategic form of the decision problem. That is, we can set out the strategies for Chooser and Demon, and say what payout Chooser gets for each possible pair of choices.

|          | PO  | PE  |
|:--------:|:---:|:---:|
| **S~1~** |  2  |  2  |
| **S~2~** |  2  |  2  |
| **S~3~** |  0  |  1  |
| **S~4~** |  3  |  1  |

: The strategic form of @fig-stalnaker-centipede. {#tbl-stalnaker-centipede}

If Chooser was making a one-off choice in @tbl-stalnaker-centipede, it would be rational, according to GDT, to choose S~2~. That is ratifiable and not weakly dominated. It wouldn't be rational to choose S~1~, because given S~1~ it would be better to choose S~4~. But given S~2~ is chosen, it is optimal, since once S~2~ is chosen, Chooser should believe that Demon is playing PE. So in this one-shot game, where one just chooses a strategy, it is rational (according to GDT) to play S~2~.

This means that GDT does not endorse what's sometimes called strategic form-extensive form equivalence. It is not the case, according to GDT, that all that matters in an extensive form decision problem, like the tree in @fig-stalnaker-centipede, is the rational choice of strategy. Sometimes a rational strategy might commit one to playing irrational moves down the track, and that's ruled out.

## Old Text

What about the other direction? Is it sensible to have a sophisticated theory that is not resolute? There does seem to be something puzzling about such theories. They are "diachronically exploitable" in the sense described by @Spencer2021. Let's start with one example. Extend the theory offered by @Gallow2020 to make it a pure sophisticated dynamic theory. That is, in a decision tree, the chooser values future choices by the expected value of the choice they'll make, and if that choice is guaranteed to end the decision tree, they use Gallow's theory. Chooser is now offered the following two-step option. At step 1 they can choose to receive 1 or play the game in @tbl-gallow-sophisticated.

|       |        |        |
|:-----:|:------:|:------:|
|       | **PU** | **PD** |
| **U** |   2    |   2    |
| **D** |   5    |   0    |

: A challenge for pure sophisticated decision. {#tbl-gallow-sophisticated}

If Chooser gets to step 2, they'll play D, since it is the best option according to Gallow's theory. So at step 1 they'll choose the 1 rather than playing this game. But that's absurd; they know they could have done better by simply playing the game and choosing U.

What the Dual Mandate says is that the last step of reasoning here is sound; it is a fair criticism of an agent to say that their strategy doesn't make sense even if every step makes sense taken on its own. Since this does seem like a fair criticism, it is reasonable to adopt the Dual Mandate.

If one has a decisive theory, then a huge number of decision trees will be dilemmas, since it is unlikely that the optimal strategy matches the series of optimal choices. This is not a reason to reject the Dual Mandate; it's another reason to reject decisiveness.

You might worry that the argument based around @tbl-gallow-sophisticated is not really an objection to theories that reject the Dual Mandate, but just to the combination of that rejection and the endorsement of Gallow's particular theory of decision. That worry is half right. This result is a problem for Gallow's theory. But that doesn't mean it isn't also an argument for the Dual Mandate. The point of the Dual Mandate is not to criticise individual decisions, like taking the 1 in this game. It's to criticise theories that endorse those decisions. It's true that once we find the right theory of synchronic choice, the Dual Mandate will be unnecessary, since it will be automatically satisfied.[^dual-old-9] But the Dual Mandate plays an essential role in selecting that theory.

[^dual-old-9]: IMPORTANT NOTE TO SELF: This isn't right. In cases where there are multiple equilibria, earlier choices might rule out some later choices. E.g., when there is an exit choice that is guaranteed to be better than some earlier choice. Gotta fix all this.

Jack @Spencer2021 has an example which he thinks tells against the Dual Mandate, or what he calls the requirement that Chooser not be diachronically exploitable.[^dual-old-10] The agent will play first the left and then the right game, and their payouts (shown in dollars) will be summed over the game. They won't be told between the games what they got from the first game.[^dual-old-11]

[^dual-old-10]: Spencer's non-exploitability isn't quite the same thing as the Dual Mandate, but it's close enough for these purposes. Spencer rejects non-exploitability, but endorses a weaker constraint he calls the Guaranteed Principle. I don't see any reason to distinguish between these constraints, in part because of the argument that follows in the text.

[^dual-old-11]: Assume Chooser is reasonably risk-neutral over dollars over this range of outcomes.

::: {#tbl-newcomb-insurance layout-ncol="2"}
|          |           |           |
|:--------:|:---------:|:---------:|
|          | **PU~1~** | **PD~1~** |
| **U~1~** |    50     |    -50    |
| **D~1~** |    60     |    -40    |

: First game {#tbl-ni-left}

|          |             |               |
|:--------:|:-----------:|:-------------:|
|          | **Correct** | **Incorrect** |
| **U~2~** |     25      |      -75      |
| **D~2~** |     -25     |      75       |

: Second game {#tbl-ni-right}

Ahmed Insurance (from @Spencer2021).
:::

Note that in @tbl-ni-right, the states are not the usual ones about Demon's predictions. Rather, they are that the Demon made the Correct, or Incorrect, prediction in @tbl-ni-left. There are eight strategies in this game, but since the Demon doesn't care about what happens at non-chosen nodes, we won't care either, and just focus on the four combinations of moves Chooser might make, and how they interact with Demon's prediction. If we do that, we get the following table (also given by Spencer, and also with payouts in dollars).

|              |           |           |
|:------------:|:---------:|:---------:|
|              | **PU~1~** | **PD~1~** |
| **U~1~U~2~** |    75     |   -125    |
| **U~1~D~2~** |    25     |    25     |
| **D~1~U~2~** |    -15    |    -15    |
| **D~1~D~2~** |    135    |    -65    |

: Strategic form of Ahmed Insurance. {#tbl-ni-strategic}

Spencer argues that even though D~1~U~2~ is dominated by U~1~D~2~ it might be rational to play it. After all, it is rational to bet on Demon being correct in @tbl-ni-right, since Demon is arbitrarily good. And if one knows one is going to do that, one may as well take the sure extra \$10 that playing U~1~ rather than D~1~ gives. So diachronic exploitability is consistent with rationality.

The reasoning of the previous paragraph fails because neither CDT, nor any other sensible decision theory, recommends taking two boxes in Newcomb Problems embedded in strategic interactions. This would be like thinking that CDT recommended always defecting in Iterated Prisoners' Dilemma, even it was chancy whether the iterations came to an end after each round, so backward induction reasoning was unavailable. If Chooser has convinced themselves that they will play U~2~, and we'll come back to whether they should believe that, then the choice in @tbl-ni-left comes down to this.

|          |           |           |
|:--------:|:---------:|:---------:|
|          | **PU~1~** | **PD~1~** |
| **U~1~** |    75     |   -125    |
| **D~1~** |    -15    |    -15    |

: First game in Ahmed Insurance, if D~2~ will be played. {#tbl-ni-new-left}

This game has two pure strategy equilibria, and on its own I think (because of the arguments in @sec-indecisive) that either play is acceptable. In context though, either play is clearly unacceptable. Given that one chooses either U~1~ or D~1~, the only reasonable thing to believe is that Demon has almost certainly predicted this, so it makes to play U~2~, since Demon is almost certainly correct. So one ends up playing U~1~U~2~ or D~1~U~2~, both of which are dominated and hence absurd strategies.

Spencer argues that since the Demon is almost certainly accurate, Chooser should play U~2~, so they should play a dominated strategy, so the Dual Mandate doesn't apply. (This assumes that synchronic choice rules out strictly dominated options in cases like this, but Spencer agrees that it does.) This argument only goes through if Chooser doesn't have access to mixed strategies; i.e., if Chooser is not ideally practically rational. If Chooser does have access to mixed strategies, they should play a 50/50 mix of U~1~ and D~1~, then choose D~2~. That is ratifiable as long as Chooser believes Demon plays PU~1~ with probability 0.45, and PD~1~ with probability 0.55. Since that's the only ratifiable play for Demon, it's reasonable for Chooser to believe this. If mixed strategies are allowed, this is not a case where the Dual Mandate fails.

In general, if mixed strategies are not allowed, the Dual Mandate is implausible. But that's because without mixed strategies, cases like @tbl-newcomb-insurance are dilemmas; they have no ratifiable choices. And it's true that the Dual Mandate is implausible in dilemmas. Think back to the sinner described in @sec-ratify. Imagine that sinner will in fact say that they get *d* days in heaven. Now complicate the case; they are offered a choice of *d*! days in heaven, or to make their own choice. If they will in fact choose *d*, they should simply take *d*!, even though there are strategies available, like choosing *d*!! days, that are better. Weird things happen when there are dilemmas around, and we shouldn't judge decision theories against these cases.

The Dual Mandate is also implausible if Chooser thinks they will be irrational, or that they will have different preferences. Indeed, it is implausible if Chooser thinks they might either change or lose their mind. For example, Odysseus binds himself to the mast because he does not approve of future-Odysseus's preferences. Professor Procrastinate[^dual-old-12] cite turns down a referee request because he does not trust his future self to be practically rational. Both of them deliberately turn down strategies that would be better than where they end up, because they do not trust their future selves to carry them out. They are alienated in this way from their future selves. When one does not endorse one's future preferences, or does not trust one's rationality in the future, it makes sense to be alienated from one's future self in this way. In such cases, one's future self is just another part of the world that must be predicted and worked around. And so it might make sense to forego, as Odysseus and Procrastinate forego, strategies that one's future self will not be so kind as to carry out.

[^dual-old-12]: A famous character in @JacksonPargetter1986.

My main claim here is when neither of those two conditions obtain, i.e., when one knows that one's future self will be rational and have the same preferences, one's choices should make strategic sense. That is, they should satisfy the fairly weak condition that they are part of some strategy that one could choose if one was simply choosing a strategy for the whole tree. Unless one fears future irrationality, or future change of preference, one should not be alienated from one's future self. If Chooser takes 1 rather than play @tbl-gallow-sophisticated, they are alienated in this way. They have to think, I know I'd be better off if I played U. But that fool future-me will play D instead, and blow up the plan. But future-them is not a fool; by hypothesis they are known to be ideally rational. So it isn't coherent to think this way, and that reveals that it is incoherent to 'rationally' take the 1. And that is why the Dual Mandate requires that one's strategy be rational, and not just the moves that make up the strategy.
